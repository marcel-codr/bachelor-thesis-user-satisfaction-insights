%% This is file `DEMO-TUDaThesis.tex' version 2.09 (2020/03/13),
%% it is part of
%% TUDa-CI -- Corporate Design for TU Darmstadt
%% ----------------------------------------------------------------------------
%%
%%  Copyright (C) 2018--2020 by Marei Peischl <marei@peitex.de>
%%
%% ============================================================================
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2008/05/04 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainers of this work are
%%   Marei Peischl <tuda-ci@peitex.de>
%%   Markus Lazanowski <latex@ce.tu-darmstadt.de>
%%
%% The development respository can be found at
%% https://github.com/tudace/tuda_latex_templates
%% Please use the issue tracker for feedback!
%%
%% ============================================================================
%%
% !TeX program = lualatex
%%

\documentclass[
	english,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=1b,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=1mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
	DIV=14,
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}
\usepackage{datetime2}


% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
%\usepackage{iftex}
%\ifPDFTeX
%	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
%\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman, main=english]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}
\usepackage[super]{nth} % converts Dec \nth{1} to Dec 1st (with st as superscript)


%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage[style=authoryear-ibid,backend=biber]{biblatex}   % Literaturverzeichnis
\DeclareLanguageMapping{english}{english-apa}
\bibliography{references}
\AtBeginBibliography{\setcounter{maxnames}{10}} % Im Literaturverzeichnis nenne mehr als 2 Autoren
\counterwithout{footnote}{chapter} % Zähle Fußnoten über gesamtes Dokument hoch


%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
%\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
\usepackage{longtable} % Mehrseitige Tabellen
\usepackage{xtab,afterpage}
%\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien
\usepackage{tikz}       % Graphen zeichnen
\usepackage{enumitem, hyperref}   % for referencing in enumerate und description
\usepackage{amsmath}    % https://tex.stackexchange.com/questions/32140/how-to-write-a-function-piecewise-with-bracket-outside
% https://texblog.org/2012/03/21/cross-referencing-list-items/
\usepackage[acronym,shortcuts,nonumberlist,nomain]{glossaries} % for Abbreviations (Acronyms)

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

% Customs
%\usepackage[singlespacing]{setspace}
\usepackage[]{setspace}
%\usepackage[printonlyused]{acronym}

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}

\makeglossaries

\begin{document}

\Metadata{
	title=Automatic evaluation of Enterprise-GenAI Applications success,
	author=Marcel Pfeiffer
}

\pagenumbering{gobble}

\title{Automatic evaluation of Enterprise-GenAI Applications success}

\studentID{2912332}
\author[M. Pfeiffer]{Marcel Pfeiffer}%optionales Argument ist die Signatur,
\birthplace{Koblenz}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Prof. Dr. Peter Buxmann \and Prof. Dr. X Y}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
\department{wi} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Wirtschatsinformatik}
\group{Software \& Digital Business}

\submissiondate{\today}
\examdate{\today}

%	\tuprints{urn=1234,printid=12345}
%	\dedication{Für alle, die \TeX{} nutzen.}

\maketitle

\affidavit


%---------------------------------------------------------------------%
%----------------------------- Abstract ------------------------------%
%---------------------------------------------------------------------%
\chapter*{Abstract}
Der Abstract fasst den Inhalt der Abschlussarbeit auf maximal einer halben Seite zusammen. Ziel des Abstracts ist es, dem Leser einen möglichst schnellen Überblick über den Inhalt der Arbeit zu verschaffen und die wesentlichen Ergebnisse darzustellen. Mögliche Inhalte sind die Zielsetzung oder Forschungsfrage der Arbeit, verwendete Theorien, die verwendete Methodik sowie eine kurze Darstellung der zentralen Ergebnisse und Beiträge.

\tableofcontents


\pagenumbering{Roman}

%---------------------------------------------------------------------%
%-------------------------- List of Figures --------------------------%
%---------------------------------------------------------------------%

\listoffigures
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen
%---------------------------------------------------------------------%
%--------------------------- List of Tables --------------------------%
%---------------------------------------------------------------------%

\listoftables
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen

%---------------------------------------------------------------------%
%----------------------- List of Abbreviations -----------------------%
%---------------------------------------------------------------------%
\chapter*{Abkürzungsverzeichnis}
\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen

\begin{xtabular}{ll}
IS					&	Information Systems\\
TUD					&	Technische Universität Darmstadt\\
KMS                 &   Knowledge Management System \\

\end{xtabular} 

%---------------------------------------------------------------------%
%---------------------------- Introduction ---------------------------%
%---------------------------------------------------------------------%

\onehalfspacing
\chapter{Introduction}
% Umstellen auf arabische Seitenzahlen. Die Anzahl der bisherigen
% Seiten wird zwischengespeichert, da die römische Nummerierung am
% Ende des Dokuments fortgesetzt werden soll.
\newcounter{seitenzahlroemisch}
\setcounter{seitenzahlroemisch}{\value{page}}
\pagenumbering{arabic}

Die Datei \file{DEMO-TUDaThesis.tex} ist ein Template für Abschlussarbeiten im Stil des Corporate Designs der TU Darmstadt.
Sie ist Teil des TUDa-CI-Bundles wurde vom in Teilen tuddesign-Paket von C.~v.~Loewenich und J.~Werner inspiriert.

 \section{Abbildungen}
Abbildungen sollten generell linksbündig in das Dokument eingefügt werden. Die Abbildungs-beschriftung steht dabei unter der Abbildung und ist ebenfalls linksbündig auszurichten. Eine Beschriftung der Abbildungen mit der Abkürzung "Abb." ist ebenfalls möglich. Abbildungen (bzw. ihre Beschriftungen) werden von einer Leerzeile gefolgt, um den Abstand zum nachfolgenden Text zu wahren.

Beispiele:

 
\begin{figure}[ht]
     \centering
     \includegraphics[]{images/Picture 1.png}
     \caption{Logo des Fachgebiets Wirtschaftsinformatik}
     \label{fig:logo}
\end{figure}

\begin{figure}[ht]
     \centering
     \includegraphics[]{images/Picture 1.jpg}
     \caption{Video-Podcast der Vorlesung Internet Economics auf einem iPod}
     \label{fig:logo2}
\end{figure}

\section{Zitierungen}
Wörtliche Zitate, die sich über mind. drei Textzeilen erstrecken, sind mit der "Zitat" zu formatieren. Alle anderen Zitate erhalten die gleiche Formatierung wie der restliche Fließtext \parencite{buxmann2015softwareindustrie}.

%---------------------------------------------------------------------%
%------------------------------ Theoretical Background ---------------%
%---------------------------------------------------------------------%
\chapter{Theoretical Background}
In order to gain a deeper understanding of what determines the success of an enterprise Gen-AI system, we will first look at the success of IS in general. Afterwards, we will dive deeper into the context of KMS and specifically enterprise GenAI systems and apply the more general theories to them. This will help us define and categorize the software features we are going to implement out during our artifact development.
\section{Technology acceptance and IS success}
To better understand the success of IS we will have a look at two models. The IS success model \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} focuses on the success of an IS in an organizational context, whereas the technology acceptance model (TAM) deals with the question, which determinants lead to the acceptance and use of an IS by a single user.
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{images/TAM.png}
    \caption{Technology acceptance model}
    \label{fig:enter-label}
\end{figure}
\\
Looking at the TAM there are two determinants that influence the use of an IS - the \textit{perceived usefulness} (PU) and the \textit{perceived ease of use} (PEU). The PU describes the degree to which a person believes that using a particular system would provide some use for her or him \parencite{Davis1989MISQ}. In an enterprise context this usually means the conviction that the use of the IS has a beneficial influence on the own job performance. The second determinant, the \textit{perceived ease of use} (PEU), refers to "the degree to which a person believes that using a particular system would be free of effort" \parencite[p.~320]{Davis1989MISQ}. This idea relates to the user's assessment of the mental effort required to learn and operate the system. If a system is intuitive, straightforward and does not require extensive training to use, it will be perceived as easy to use.\\ A crucial aspect of the TAM is the relationship between these two determinants. Davis argues that the PEU has a direct, positive influence on the PU. A system that is easier to use is also perceived as more useful, partly because the user can invest less effort in the mechanics of using the system and more in the actual task at hand. Both PU and PEU then influence the user's \textit{attitude toward using} the system, which in turn shapes the \textit{behavioral intention to use} and ultimately leads to the \textit{actual system use}.However, it is important to note that a consistent finding in TAM research is that PU has a significantly stronger influence on usage intention and actual use than PEU \parencite{Davis1989MISQ}. While ease of use is important, the ultimate driver for acceptance is the user's belief that the system will help them perform their job better.\\
\\
Now that we know which factors lead to the use of an IS by a single person, we will shift our perspective to the factors that determine its use and overall success within an organizational context.
Following DeLone and McLean \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} there are 6 interrelated factors that determine the success of an IS in an organizational context: \textit{system quality, information quality, service quality, intention to use and actual system use, user satisfaction and the net benefits} of the IS.\\
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{images/ISSuccess.png}
    \caption{IS Success Model}
    \label{fig:enter-label}
\end{figure}
\\
\textit{System quality} means the quality of the IS in terms of technical performance. Typical measures for system quality would be, for example, response time, reliability, ease of use, or hardware utilization.\\
\textit{Information quality} refers to the quality of the information provided by the IS. This includes, for example, the usefulness, timeliness, precision, completeness, and relevance of the information provided by the system.\\
\textit{Service quality} means the quality of the support that users receive from the IS department or service provider. It became an important success dimension with the rise of end-user computing, where the IS function provides services in addition to products. This dimension is often measured by the support staff's responsiveness, assurance, and empathy.
\\
These three aspects of the quality of an information system directly influence the \textit{intention to use} and the actual \textit{system use} as well as the \textit{user satisfaction}. Here, it is important to differentiate between the two concepts of use. \textit{Intention to use} is an attitudinal measure, representing a user's plan or predisposition to engage with a system. In contrast, \textit{system use} is a behavioral measure of the actual interaction, capturing metrics like frequency, duration, or the breadth of features used. This distinction is particularly crucial in an enterprise context. For a system where use is voluntary, a user's intention is a strong predictor of their actual use. However, if a system's use is mandatory, simply measuring that it is being used is not very insightful. In that scenario, assessing the nature of the use (e.g., are users leveraging advanced features?) or their underlying intention to use the system properly becomes a more telling indicator of true acceptance 
\parencite{DeloneMcLean2003ISSuccessTenYearUpdate}. Both use and intention are linked with \textit{user satisfaction}. A positive experience during system use leads to higher satisfaction, which in turn reinforces the intention for continued and more extensive use in the future. In an organizational context, this user satisfaction is heavily influenced by the system's perceived impact on job performance, such as an increase in productivity. Therefore, the combination of active system use and high user satisfaction is what generates \textit{net benefits}.\\
This process, however, is not a one-way street. The model includes a crucial feedback loop: the \textit{net benefits} that users perceive will in turn influence their future \textit{user satisfaction} and \textit{intention to use}. When users see that a system is delivering real value, it reinforces their positive attitude towards the system. This creates a virtuous cycle, where a successful system's value is constantly reinforced and grown over time.
\newpage
\section{Knowledge Management Systems acceptance success}
Now that we have a solid understanding of IS acceptance and success in general we are going to have a closer look at knowledge management systems (KMS) in particular enterprise GenAI systems. We'll start of with some definitions that will help us understand what KM and KMS are. "Knowledge is an evolving mix of framed experience, values, contextual information, and expert insight that provides a framework for evaluating and incorporating new experiences and information". This means that knowledge is a key key factor in the devision-making processes e.g. in enterprises. Knowledge Management on the other hand "is the practice of selectively applying knowledge from previous experiences of decision making to current and future decision making activities with the express purpose of improving the organization’s effectiveness". KMS in the end can be defined as "IT-based systems developed to support and enhance the organizational processes of knowledge creation, storage/retrieval, transfer and application". Overall we can conclude that KMS are systems that support the decision making process e.g. in a company by managing and selectively providing knowledge to their users. Since KMS are by definition a specialized form of IS we can simply apply the ISSM to them.\\
Following Jennex and Olfman we can extend the three quality factors to gain a better understanding
of knowledge management success.\\
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{knowledge_management_success.png}
    \caption{Knowledge management success model by Jennex & Olfman}
    \label{fig:enter-label}
\end{figure}
The \textit{System Quality} dimension now extends to contain the \textit{technological resources}, \textit{KM form} and the \textit{KM level}. \textit{Technological resources} define the capability of an organization to develop, operate, and maintain KM. This includes all kinds of technical conditions like the amount of experience available to develop, operate and maintain the KM, the underlying network infrastructure or the databases to hold the data use in KM. The \textit{KM form} refers to the extent to which knowledge and knowledge management processes are computerized and integrated. This is important since computerized KM processes are more effective than analogue ones as they make knowledge accessible everywhere at any time through a simple to use interface. The \textit{KM level} in the end refers to the ability to influence current activities. \textit{Technological resources} and the \textit{KM form} are both enablers for the \textit{KM level} and serve as a technical foundation for the ability of the KMS to influence the decision-making process.\\\\
The \textit{Knowledge Quality· dimension} consists of three subsidiary factors too - the \textit{KM strategy/process}, the \textit{Richness} of the captured knowledge and the \textit{Linkages} among the knowledge components. The \textit{Knowledge strategy/process} factor is the basis for the two other factors and means looks at the organizational processes organizing the knowledge management and to which extend these processes are formalized. \textit{Richness} on the other hand evaluates the accuracy and timeliness of the knowledge itself - one could say it is the quality of the actual knowledge. Linkages refer to how accessible the knowledge is through things like knowledge and topic maps etc.\\\\
The \textit{service quality} dimension determines if users of the KMS are provided with sufficient support to use the KMS effectively. It consists of three parts aswell. The \textit{management support} refers to the provision of adequate resources for the KM the organization aswell as the creation of knowledge-sharing organization culture by the organization's management. User KM service quality refers to the support provided by the user organization in order to train the users to use the KMS effectively. On the other hand the IS KM service quality refers to the support provided by the  IS organization to allow users to use the KM aswell as maintaining it. This includes the building, setup and maintaining the KM infrastructure ensuring availability, reliability and security of the KMS provided.\\\\
All three dimensions - the \textit{system quality}, \textit{knowledge quality} and the \textit{service quality} influence similar to the original ISSS the \textit{Intention to use and usage} aswell as the \textit{User Satisfaction} and cause a \textit{net benefit} in the end.
\section{RAG Systems in enterprises}
Since we now know what determines the success of KMS in organizations like enterprises, we are going to have a closer look at the GenAI systems that are used to support KM in an organization, find out what they are used for and which factors define their quality.\\
The GenAI system that we are going to focus on throughout this thesis is a retrieval augmented generation (RAG) system. These systems utilize external data sources like web pages, other organizational KMS or third-party sources through retrieval algorithms, and pass the retrieved knowledge to large language models (LLM) that generate answers grounded on the retrieved knowledge. As such RAG systems can be used for knowledge retrieval and provide advantages for this task.\\
One problem of LLMs that do not use RAG is that they can only provide the data that they were trained on. This causes them to become out-dated. As we know from the KMS success model it is crucial for an organization to be able to access up-to-date information using their KMS. RAG systems can provide this by utilizing up-to-date knowledge sources for their generation.\\
Another issue of LLMs is that they tend to hallucinate if they don't know the answer to a users query or apply a pattern from the training data in a wrong way. Since RAG systems bound the LLMs they use to the information provided by the retrieval algorithms, they reduce hallucinations and a user is always able to verify the answer provided by the LLM by double-checking the sources that were used by the RAG system. This way the RAG approach improves the accuracy of LLMs.\\
Since RAG systems are able to query various data sources at once, they provide good linkages to other KMS as well. Following the KM success model RAG systems seem like a pretty good choice when it comes to KM.\\
However, RAG systems need some effort to serve these qualities to their users in the organizations practice. One of the major challenges is the underlying data that is used for the RAG system. These data are usually not optimized for the use by a RAG system which poses a lot of challenges to the operators of the system. One example for these challenges are vast and complex knowledge bases that on the one hand lead to poor performance - bad \textit{system performance} - since the retrieval algorithms have to perform on a huge dataset and on the other hand pose challenges regarding the integration of them (\textit{service quality}) since various APIs have to be connected to the RAG system. Enterprises usually need a very high accuracy as well because their decision-making processes involve important legal or financial decisions and if a RAG system fails to meet the required accuracy, e.g. because the retrieval algorithms are not able to gather the requested information from the huge and complex dataset, this can lead to a poor \textit{net benefit} of the system. RAG systems like any other GenAI technology are new to most end users too and require training \textit{(service quality)} in order to allow users to use them effectively. In the end we can conclude that RAG systems provide a huge potential at enhancing an organizations KM, but they need a lot of effort to realize it. The goal of this thesis is to support organizations with the realization of RAG applications potential by providing system administrators valuable insights into the usage of the systems.\\
%---------------------------------------------------------------------%
%------------------------------ Methodology --------------------------%
%---------------------------------------------------------------------%
\chapter{Methodology}
The research methodology is based on the DSR approach, which involves both the development and the evaluation of artifacts in iterative cycles. DSR aims to develop innovative solutions for specific problems and to test these solutions in real-world contexts (Peffers et al., 2007b, p. 4).\\
\subsection{Problem Identification and Motivation}
In the first phase of the DSR approach, the need for support in managing the RAG system was identified. It was determined that system administrators require insights that allow them to better assess the current performance and quality of the RAG application and to draw conclusions about which measures must be taken to improve it. Examples of such measures could be employee training, improving the knowledge base, or enhancing the RAG mechanism itself.
\section{First iteration - Definition of valuable insights}
In the first iteration, specific software features were defined to support system administrators in managing the system. This was done in collaboration with the developers of the RAG software under investigation during several joint workshops.
\subsection{Define Objectives of Artifacts}
In a first workshop, initial ideas for evaluating the RAG application were collected, and its current data situation was analyzed to determine what additional data must be gathered to implement the automated analysis. The developers were presented with the foundational principles of Information Systems Success (ISSS) and Knowledge Management Systems (KMS) success. They then developed ideas related to both topics in small groups and afterwards discussed their findings in a plenary session. Through this process, the objectives for the evaluation of the RAG system were clearly established
\subsection{Development of Artifacts}
Based on the developers' ideas, specific concepts for software features were developed and then classified into the quality categories of the IS Success Model (ISSM).
\subsection{Demonstration and Evaluation}
In a second workshop, the developed concepts were presented to the developers and feedback on them was gathered. Additionally, the individual software features were prioritized with the categories low, medium and high to ensure that the subsequent implementation phase could focus on developing and delivering the most important ones.
\section{Second iteration - Implementation of insights}
In the second iteration, the developed software features were technically implemented and integrated into the existing RAG application.
\subsection{Define Objectives of Artifacts}
First, various solutions for the technical implementation were developed and presented once again to the development team. Feedback was gathered, and a decision was made on how the features should be technically implemented.
\subsection{Development of Artifacts}
Subsequently, the software features were implemented and integrated into the existing RAG application. This was done using an iterative software development process (Scrum). Technical reviews took place to validate the correctness of the created software features.
\subsection{Demonstration and Evaluation}
Finally, the created software features were presented to the development team in a final workshop and feedback was gathered.
%---------------------------------------------------------------------%
%------------------------------ Results ------------------------------%
%---------------------------------------------------------------------%
\chapter{Results}
In the following chapter, we are going to elaborate on the concepts for software features created during the workshops and how they have been implemented in the RAG application. Feedback from the developer team and their prioritization of the features will be discussed as well.
\section{Description of the RAG system used in the study}
To provide a basis for further investigation, this section describes the features of the Retrieval-Augmented Generation (RAG) system under examination. The system is designed for enterprise use, making company-specific knowledge accessible to employees through a conversational interface powered by LLMs.

\subsection{System Architecture and Technology Stack}
The system is deployed on the Google Cloud Platform (GCP) and utilizes Vertex AI Search for the initial document retrieval from its knowledge base. The relevance of the retrieved documents is subsequently optimized by the Vertex AI Reranker. For the generation component, the system is capable of integrating various LLMs through the LangChain framework. Given its deployment on GCP, Google's Gemini models are typically employed for response generation.

A central architectural feature of the system is the implementation of distinct \textbf{Use Cases}. This allows the system to provide context-aware responses by augmenting the generation prompt with use-case-specific information, such as a specialized glossary or other supplementary data like a use-case-specific tonality. Each Use Case is associated with a specific selection of \textbf{Knowledge Assets}, which are custom compilations of \textbf{Knowledge Sources}. A Knowledge Source contains information on a particular topic and is technically linked to a Datastore in Vertex AI Search, which performs the underlying retrieval.

\subsection{Retrieval and Query Enhancement Features}
To increase the quality of the generated answers, the system incorporates several features to enhance the retrieval process:

\begin{itemize}
    \item \textbf{Query Splitting:} The user's search query is semantically decomposed into multiple sub-queries. A separate retrieval process is executed for each sub-query, thereby improving the comprehensiveness and quality of the search results.
    
    \item \textbf{Query Reformulation:} The system supports the reformulation of queries, which enables more effective handling of follow-up questions that are related to previous questions and generally leads to an increase in retrieval quality.
    
    \item \textbf{Metadata Filtering:} The search performed by Vertex AI Search can be refined by applying a metadata filter. This allows for the retrieval to be constrained to only those documents that match specific attributes enhancing the retrieval performance since the retrieval process can operate on a smaller search space.
\end{itemize}

\subsection{User Interface and Interaction Features}
The system provides a range of interactive features to enhance the user experience and workflow integration:

\begin{itemize}
    \item \textbf{Question Suggestions:} After an answer is generated, the system proposes alternative and potential follow-up questions to the users initial question that the user can select with a single click.
    
    \item \textbf{Document Viewer:} The documents retrieved during the search process can be viewed in an integrated file viewer and are available for download too.
    
    \item \textbf{Feedback:} Users can provide individual feedback on their satisfaction with the LLM responses.

    \item \textbf{Copy Function:} Users can copy the answer provided by the LLM using a Copy Button
    
    \item \textbf{Quick Actions:} The system offers "Quick Actions," which are clickable shortcuts that allow users to perform subsequent tasks with the generated information, such as creating an email or generating a summary.
\end{itemize}
\section{Software features}
This chapter presents the results of the developer workshops, namely the resulting concepts and their contribution to the GenAI success evaluation framework, as well as the prioritization and feedback by the developer team.
\subsection{Measurements of the system quality}
\subsubsection{Response Time Measurement}
The first concept for determining the system quality of the RAG application is the measurement of its response time, defined as the duration from the submission of a user's query to the delivery of the complete answer. For a comprehensive analysis, this measurement should incorporate not only the raw duration but also key system factors to assess their impact. These factors include the specific language model (LLM) being used, and whether features such as Query Splitting or Query Reformulation were activated for the request.

For the visualization of this data, a time-series diagram is proposed. The x-axis would represent the time and the y-axis the measured response time. To improve readability and highlight long-term developments, the chart should be smoothed, for instance, by displaying daily averages. The interface should also include interactive filters that allow for the inclusion of the different system factors. These filters would enable users to display data subsets, such as showing only the response times associated with a specific LLM or with Query Splitting enabled/disabled.

The objective of this feature is to empower Use Case administrators to identify performance trends and retrospectively evaluate the impact of their own decisions on how to adapt the RAP sytem on response times. Furthermore, it allows them to better assess how different system factors influence the response time. These insights can, in turn, be used to train employees on the effective use of the system, for example, by providing better guidance on the performance implications of choosing a particular language model.

During the workshop, the development team assigned a \textbf{medium priority} to this feature. In their feedback, they noted that while the feature would be straightforward to implement, they suspected that overall response times would not vary significantly. Therefore, they hypothesized that this analysis would likely not yield major conclusions.

\subsubsection{Error Rate Monitoring}
A second concept proposed to evaluate system quality focuses on monitoring error rates to assess system stability and reliability. The core metrics to be captured are the absolute count and the percentage of user queries that result in a system error. For a more granular analysis, these error metrics can be combined with the system factors (LLM selection and the activation state of Query Splitting and Reformulation) and with the different predefined error types and messages of the system.

The visualization proposed for these data is again a time-series diagram. The x-axis would represent time, while the y-axis would display the amount or percentage of system errors. The interface is designed to be interactive, featuring filtering options for both system factors and error types. This allows for detailed analysis, such as comparing the error rates of different language models or analyzing the impact of query splitting and query reformulation on error rates.

The workshop feedback showed different views on this feature's usefulness and priority. One group assigned it a \textbf{high priority}. They argued that system errors are a major problem that can stop people from using the application and cause them to lose interest. This view highlights the feature's role in keeping users engaged. A second group assigned it a \textbf{medium priority}, seeing it mainly as a diagnostic tool for developers. For them, the main benefit was helping the development team find and fix technical issues faster. This group strongly recommended that this monitoring view should be for internal use by the development team only. They reasoned that showing raw error statistics to Use Case administrators or end-users could damage their trust in the system, which is not in the interest of the RAG systems vendors. In the end, the feature was assigned a \textbf{medium priority} since the arguments from the critical perspective predominated the other ones.
\subsection{Measurements of the knowledge quality}
\subsubsection{Topic Map}
To better understand what information users are looking for, the concept of a "Topic Map" was developed. This feature is designed to visualize the main topics that appear in the RAG system. The map can display topics from three different data sources: the user queries themselves, the content of the documents retrieved by the system, or the knowledge assets that were accessed.

The map is displayed as a bubble plot, where each bubble represents a specific topic. The positioning is semantic, meaning that topics with similar meanings are positioned closer to each other, and related topics are grouped by color. The size of each bubble shows how frequent or prominent a topic is. The feature also includes a time filter, which allows administrators to see how user interests and queried topics change over a specific period.

The development team rated the overall Topic Map feature as a \textbf{high priority}, but they assigned different levels of importance to analyzing each of the three data sources:

\begin{itemize}
    \item \textbf{Topics from user queries:} This received a \textbf{high priority}. The team agreed that this view would provide the most valuable insights into what knowledge users are actively searching for, which lets use-case-administrators identify their interests and potential knowledge gaps in the system.
    
    \item \textbf{Topics from retrieved documents:} This was given a \textbf{medium priority}. The feedback noted that because documents can be very large and cover many subjects, the detected topics might not be very precise, making this analysis less useful.
    
    \item \textbf{Topics from knowledge assets:} This was rated as a \textbf{low priority}. The team reasoned that use-case-administrators already know what their knowledge assets are about, so a visualization of these topics would likely offer little new insight.
\end{itemize}
\subsubsection{Task Type Analysis}
To analyze user intent and behavior, the concept of a "Task Type Map" was introduced. In contrast to the Topic Map, which focuses on \textit{what} users ask, this feature classifies queries based on \textit{how} users interact with the system. The primary goal is to understand the different ways the application is used, which can provide valuable insights for improving user workflows.

The initial concept proposed displaying this data in a bubble plot, similar to the Topic Map, where bubble size would represent the frequency of each task type. However, the team questioned this visualization during the workshop. They pointed out that the semantic relations of the different task types do not matter in this case since only predefined task types will be assigned to the user queries which makes a semantic interpretation of the different task types themselves obsolete. As a more practical alternative, the team suggested that a \textbf{bar chart} would be better suited to compare the frequencies of these distinct task categories.

Despite the discussion on the best visualization method, the development team assigned the Task Type Map a \textbf{high priority}. They agreed that understanding the primary ways the system is used is crucial. This information can help to improve and streamline common user workflows and better tailor the system to user behaviors.
\subsubsection{Quality of Retrieval and Response}
An important concept for evaluating the system is the measurement of its output quality. This feature focuses on assessing both the relevance of the retrieved documents and the quality of the generated response. The main goal is to establish metrics for how well the system performs to ensure that users receive accurate and helpful answers.

The discussion highlighted that there is no single and perfect way to measure quality automatically. Therefore, the team proposed different approaches that could suite the quality measurement:

\begin{itemize}
    \item \textbf{AI as a Judge:} Using a separate LLM or another AI model to automatically score the quality of a given response.
    \item \textbf{Binary Metrics:} Implementing simple, automated checks. For retrieval, this could mean checking if any retrieved text has a certain similarity score to the query. For the response, it could be a check to see if an actual answer to the question was provided or if the LLM responded with no answer.
    \item \textbf{User Behavior Analysis:} Using indirect metrics, such as counting the number of follow-up or rephrased questions. A high number could suggest that the initial response was unsatisfactory.
\end{itemize}

The results would be displayed in a time-series chart with the x-axis representing the time and the y-axis representing the quality of the system's responses. The system factors (the model selection, Query Splitting and Reformulation) also be included as filtering options as described before to see how they affect quality.

The development team unanimously assigned this feature a \textbf{high priority}, describing it as one of the most critical factors for the RAG success. The team emphasized that guaranteeing high-quality responses is essential for user retention - if users do not get good answers, they will stop using the system.
\subsubsection{Combined Quality and Topic/Task Analysis}
Another concept proposed in the workshop was a combination of the previous ideas on quality measurements and topic/task type analysis into a single visualization. This "Combined Quality and Topic/Task Analysis" was designed to connect the system's performance quality directly with the specific topics or task types being handled. The goal is to identify patterns and understand if the system's quality varies for different kinds of user queries.

This feature uses the bubble plot from the Topic as its foundation. The positioning of the bubbles (semantic similarity) and their color (topic clusters) remain the same, as does the sliding time window for dynamic analysis. The crucial change lies in the meaning of the bubble size. Instead of representing the frequency of a topic, the size of each bubble now represents the average retrieval or response quality for that topic. For example, a large bubble could signify a topic with consistently high-quality answers, while a smaller bubble might indicate a topic where the system often performs poorly.

The development team assigned this combined view a \textbf{high priority}. The key benefit they identified was the ability to find patterns between query types and the quality of the results. By using this map, administrators could quickly see if a specific topic area suffers from low-quality answers or if certain task types are handled better than others. This information is critical for making targeted improvements and focusing development efforts on the areas where the system is weakest.
\subsubsection{Combined Task Type and Topic Analysis}
Another concept was proposed to analyze the relationship between the topics users ask about and the tasks they are trying to perform. This feature is designed to identify the specific user intents that are associated with different topics. This provides a deeper understanding of user behavior within different knowledge domains.

For this analysis, a new visualization method was suggested: a \textbf{heatmap}. In this chart, the rows would represent the various topics identified in the system, while the columns would represent the different task types (e.g., knowledge query, summarization). The color intensity of the cell where a topic and task type intersect would indicate how frequently that specific combination has occurred. For example, a darker cell would show that a particular task is performed very often in connection with a given topic.

The team agreed that this analysis could provide "good insights" into user behavior and assigned a \textbf{medium to high priority}. Understanding which tasks are most common for high-interest topics could help administrators better structure content or create more relevant "Quick Actions" to improve the user experience for those specific topics.
\subsection{Measurements of the service quality}
\subsubsection{User Interactivity Tracking}
To evaluate the service quality of the application, a concept for tracking user interactivity was discussed. The idea is to monitor and count a wide range of specific user events that happen during a session. This approach tries to make the whole user interaction interpretable and therefore includes analyses interactions like copying an answer, viewing a source document, or providing direct feedback.

The data would be visualized in a time-series diagram, showing the frequency of these different events over a selected period. A key aspect of this concept is to also provide the use case admins with context or an interpretation of what each measurements for the different interaction types could mean. For example, a high number of users opening source documents might indicate they are actively verifying the system's answers, while frequent follow-up questions could suggest that the initial answers lacked sufficient detail.

The team assigned this feature an overall \textbf{medium priority}, which led to a detailed discussion about which specific events were the most valuable to track. A central point of the debate was data redundancy. Some team members argued for a lower priority on tracking metrics like the number of questions or raw feedback counts, as this data was already present in other specialized dashboards.

However, other members rated certain interactive events as \textbf{high priority} because of the unique insights they offer. The events considered most important were:
\begin{itemize}
    \item \textbf{Opened Source Documents:} This was rated high because it clearly shows if users find the retrieved sources relevant enough to investigate further.
    \item \textbf{Provided Feedback (Positive/Negative):} This was also seen as high priority because it is the most direct way to capture user sentiment and serves as a "good measurement" of perceived quality.
    \item \textbf{Deep-Dive Follow-up Questions:} This was considered a useful indicator of answer quality, as a high frequency might signal that the initial responses were incomplete.
\end{itemize}
Ultimately, the consensus was to focus on tracking the most insightful events that were not already available elsewhere.

A specific challenge that arose from tracking user interactivity was how to correctly interpret follow-up questions. The team needed a way to distinguish between "deep dive" questions, which indicate positive user engagement, and questions asked because the system's previous response was unclear or unhelpful. Two main approaches were considered: using a simple heuristic (e.g., assuming system-suggested "Follow-Up questions" are deep dives, while user-rephrased "alternative questions" are not) or employing an AI model as a "judge" to classify the question's intent based on context.

The recommendation from the workshop was to adopt a pragmatic hybrid strategy. The consensus was to start with the "simple distinction" method first, as it is easy to implement and has no operational cost. If this heuristic proves insufficient for reliably separating the two question types, the team could then explore using an AI judge as a second step. This approach was favored due to concerns that the AI judge would be expensive to run and might not guarantee reliable results.
\subsubsection{Knowledge Gap Detection}
The last concept proposed for measuring service quality is "Knowledge Gap Detection," which aims to identify areas where the system or the users may lack adequate knowledge. The goal is to highlight two distinct types of knowledge gaps:

\begin{enumerate}
    \item \textbf{Knowledge gap within the system:} This occurs when the topic of a user's question aligns with the topic of a relevant knowledge asset, but the system fails at retrieving relevant documents. This suggests an internal deficiency in the system's ability to retrieve the correct information, despite the knowledge theoretically existing within its accessible sources. This could be due to issues with indexing, retrieval algorithms, or the structuring of the knowledge assets themselves.
    \item \textbf{User knowledge gap:} This refers to situations where the topic of the user's question does not align with the topic of any relevant knowledge asset. This indicates that the user is searching for information that is either not present in the system's knowledge base or is not categorized in a way that allows for effective retrieval. This gap highlights potential areas where new knowledge assets might need to be created or where user training on system scope might be beneficial.
\end{enumerate}

For visualization, it is proposed to track the occurrences of these knowledge gap types over time and display them in a time-series diagram. The x-axis would represent time, while the y-axis would indicate the frequency of each gap type. This allows administrators to observe trends in knowledge gaps and identify if certain periods or topics are more prone to these issues.

During the workshops, the development team assigned a \textbf{medium to high priority} to the "Knowledge gap detection" feature, specifically emphasizing its importance for analyzing system data. The ability to identify internal system knowledge gaps was considered crucial for pinpointing areas where the RAG mechanism or the underlying knowledge base need improvement. The mechanism for checking topic alignment---whether topics "fit together"---was discussed. The proposed approach involves utilizing existing topic modeling techniques or semantic similarity measures (e.g., cosine similarity of embeddings) to compare the topics extracted from user queries, retrieved documents, and knowledge assets. If the similarity score falls below a predefined threshold, a mismatch, and thus a potential gap, is flagged. While the concept of detecting "user knowledge gaps" (point 'b') was acknowledged, it was generally viewed as less critical for immediate system improvement from a vendor perspective, leading to an overall \textbf{medium} priority for this aspect. The primary focus remains on improving the system's ability to leverage its existing knowledge effectively.
\subsection{Implementation Roadmap}
Finally, after discussion with the development team, the following roadmap for implementing the proposed software features was agreed upon:
\begin{enumerate}
    \item Implementation of the tracking system for the user interactions
    \item Implementation of a caching logic for performance optimization
    \item Topic maps
    \item Quality of retrieval/responses
    \item Combination of quality and topic/task type maps
    \item Task type maps
    \item Knowledge gap detection
    \item Combination of task types and topics
    \item Interactivity analysis
    \item Response time analysis
    \item Error rate analysis
\end{enumerate}
This roadmap was particularly important because it was clear from the beginning that not all of the proposed features could be implemented within the timeframe of this thesis due to time constraints. Therefore, prioritizing the concepts was essential to ensure that the most important features could be developed and integrated first.
\section{Description of the implemented software features}
The following section describes the software features that were implemented in the RAG system. For each feature, the description will cover its integration into the user flow as well as its technical implementation.
\subsection{Tracking system and GDPR compliance}
The foundation of the tracking system is BigQuery, a scalable, high-performance data warehouse solution from Google. As it is natively integrated into the Google Cloud Platform (GCP), it is well-suited for analyzing the application's user data.

To record the data, a Google Cloud Run service is used, which receives data from the RAG application's Vue-based frontend via HTTP and writes it to BigQuery. These HTTP calls are executed asynchronously and in parallel with the rest of the application's requests. This ensures that the tracking process does not interfere with the performance or usability of the RAG application.
The foundation of the tracking system is Google's BigQuery, a scalable, high-performance data warehouse solution. As it is natively integrated into the Google Cloud Platform (GCP), it is well-suited for the analysis of user data.

To record the data, a Google Cloud Run service is used, which receives data from the RAG application's Vue-based frontend via HTTP and writes it to BigQuery. These HTTP calls are executed asynchronously and in parallel with the rest of the application's requests. This ensures that the tracking process does not interfere with the performance or usability of the RAG application. The tracking system supports the recording of the following interaction types:
\begin{itemize}
    \item \textbf{Question Asked:} A user submits a new query.
    \item \textbf{Documents Received:} The system retrieves a set of source documents.
    \item \textbf{Answer Received:} The LLM generates a final response.
    \item \textbf{Quick Action Used:} A user uses Quick Action.
    \item \textbf{Alternative Question Asked:} A user asks an alternative question.
    \item \textbf{Follow-Up Question Asked:} A user selects a system-suggested follow-up question.
    \item \textbf{Source Document Opened:} A user opens a source document to view its contents.
    \item \textbf{Feedback Provided:} A user gives feedback on a generated answer.
    \item \textbf{Model Selected:} A user changes the language model for the conversation.
    \item \textbf{Query Reformulation Used:} The query reformulation feature is activated.
    \item \textbf{Query Splitting Used:} The query splitting feature is activated.
    \item \textbf{Copy Button Clicked:} A user copies the text of the generated answer.
    \item \textbf{Filtering Used:} A user applies a metadata filter for the search.
\end{itemize}
To maintain the context of a conversation, interactions belonging to a chat or thread within a Use Case are also tracked. In BigQuery, the chat is recorded as a "SessionChat" and the thread is recorded as a "Session". This structure makes it possible to determine which interactions were triggered within the same chat or thread.

For compliance with GDPR (DSGVO) regulations, these chats and threads are pseudonymized. This means that pseudonymized IDs are used to prevent a chat or thread from being linked back to a specific user, removing the possibility of subsequent re-identification. Furthermore, a table was created containing a justification for why the recording is necessary for every single data point that is tracked to make the tracking system GDPR compliant.
\subsection{Data flow and data preprocessing}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/DataFlow_Bachelorthesis.drawio.png}
    \caption{Dataflow}
    \label{fig:enter-label}
\end{figure}
The data flow for the dashboard system, as illustrated in the preceding diagram, can be broken down into three primary processes:

\begin{enumerate}
    \item \textbf{Data Collection:} As described before, when a user interacts with the system these interactions, such as asking a question or clicking a button, triggers an event that is sent via an HTTP request to a \textbf{Cloud Run Service}. The purpose of this service is to receive these events and insert the raw interaction data into \textbf{BigQuery}.

    \item \textbf{Data Preprocessing:} To ensure the dashboards are performant and can display analytics quickly, the data is preprocessed since some of the calculation steps have a long runtime (more details on that later). Instead, an automated preprocessing job is run. A \textbf{Cloud Scheduler} is configured to trigger a \textbf{Cloud Run Service} at configurable regular intervals. This service reads the raw data from \textbf{BigQuery}, performs the necessary calculations to prepare the data for displaying in the dashboard, and writes the processed data back into new tables within \textbf{BigQuery}.

    \item \textbf{Data Visualization:} When a user opens a dashboard in the \textbf{Vue Frontend}, a request is sent to a \textbf{Cloud Run Service} that functions as a backend for the dashboard. This service queries the prepared, preprocessed data from \textbf{BigQuery}. It then sends this data back to the \textbf{Vue Frontend}, which renders the final charts and visualizations for the user. This architecture ensures that user requests are served quickly, as the complex data processing has already been handled before.
\end{enumerate}
\subsection{The Welcome Page}
The first page a Use Case administrator sees when accessing the dashboard is the Welcome Page. This page serves as a central starting point, providing instructions on how to use the dashboard system and offering short descriptions for each of the available analytical metrics. As shown in the screenshot, the user receives guidance on the general workflow in a "Getting Started" section. Above this, a selection of cards introduces the different metrics of the dashboard. From here, the administrator can begin their analysis by selecting one of the metrics to explore.
\subsection{Answer Quality}
\subsubsection{User Flow}
The answer quality feature was implemented to provide Use Case administrators with a time-series chart, as outlined in the initial feature concept. This chart allows them to observe trends in answer quality over time and draw conclusions about the success of their own measures. The diagram displays the four quality metrics previously discussed: Relevance, Completeness, Accuracy, and Faithfulness. To provide a clearer overview, the values are smoothed to show the daily averages over the selected period. In addition to filtering by Use Case and date range, the dashboard also displays the overall average values for each metric above the chart, enabling a quick analysis of the entire period. The chart is interactive, allowing users to hide individual lines for better focus and use tooltips to analyze specific data points in detail.

\subsubsection{Technical Implementation}
During the workshops, three technical options were proposed for this feature: evaluation using binary metrics, analysis of user behavior, or using an AI model as a "judge". The binary metric and user behavior analysis were considered to be less accurate proxies for quality, as both methods only indirectly evaluate the content of the generated response. This is obvious in the case of user behavior analysis, which only looks at the user's subsequent questions, not the answer itself. The binary analysis is also limited, as it primarily checks if the RAG system could assemble relevant information but does not assess the qualitative aspects of that information.

Therefore, the decision was made to implement the AI evaluation variant. In this implementation, a language model is provided with the user's query, the retrieved source documents, and the final answer. Using a prompt template that contains detailed instructions for determining the different quality metrics, the AI "judge" then assesses the answer and calculates the scores.
\subsection{Document quality}
\subsubsection{User Flow}
The Document Quality feature was implemented in a manner similar to the Answer Quality feature, based on the concepts developed in the workshops. For each user query, this dashboard displays a time series that shows several metrics related to the retrieved documents: the average, highest, and lowest relevance scores, as well as the total number of documents. The technical basis for relevance is described in the next section. These values are presented as smoothed daily averages in a time-series chart to make it easier for the administrator to analyze trends. The dashboard also includes options to filter by Use Case and time period, and the overall averages for each metric are displayed as KPI cards above the chart. The chart itself is interactive, allowing administrators to use tooltips for detailed analysis of specific data points and to toggle the visibility of individual time series.

\subsubsection{Technical Implementation}
To determine the document quality metrics, the implementation uses the relevance scores that are calculated by the system's reranker during the retrieval process. This approach was chosen to avoid the additional costs associated with using another AI model for a separate evaluation. These reranker scores, which are generated by a cross-encoder, already provide an assessment of how well the retrieved documents match the user's query. From these scores, the system then simply calculates the average, highest, and lowest relevance value for each query, as well as the total number of documents retrieved.
\subsection{Topic maps}
\subsubsection{User Flow}
The Topic Maps are presented to the user as a bubble plot. In this visualization, each bubble represents an individual user question. The position of each bubble in the chart is determined by its semantic meaning, with topically similar questions appearing closer together than dissimilar ones. Furthermore, the questions are thematically grouped using a clustering algorithm. Each cluster is assigned a unique color in the chart and is displayed with a descriptive title in the legend. The dashboard allows administrators to filter the view by Use Case and date range, enabling them to analyze how the thematic composition of user queries changes over time. Tooltips are also available to show details for each individual question bubble.

\subsubsection{Technical Implementation}
Technically, the Topic Maps are generated by first embedding all user queries using the \texttt{gemini-embedding-001} model in its clustering mode. These embeddings are then clustered using the HDBSCAN algorithm. To achieve robust results across varying amounts of data, the input parameters for HDBSCAN, specifically \texttt{min\_cluster\_size} and \texttt{min\_samples}, are dynamically adjusted based on the total number of queries being analyzed. This adaptive approach ensures sensible clustering for both small and large datasets.

After clustering, a language model uses a prompt template to generate a descriptive title for each cluster based on a sample of its questions; this title is then displayed in the legend. Finally, to visualize the 3072-dimensional vectors produced by the embedding model, their dimensionality is reduced to two dimensions using the t-SNE technique, allowing them to be plotted on the 2D chart.
\subsection{Task type analysis}
\subsubsection{User flow}
\subsubsection{Technical implementation}
\subsection{Combining quality with topic maps}
\subsubsection{User flow}
\subsubsection{Technical implementation}
\subsection{Combining quality with task types}
\subsubsection{User flow}
\subsubsection{Technical implementation}
\subsection{Combination of task type and topics}
\subsubsection{User flow}
\subsubsection{Technical implementation}
\section{Final feedback from the development team}

%---------------------------------------------------------------------%
%------------------------------ Discussion ---------------------------%
%---------------------------------------------------------------------%
\chapter{Discussion}

%---------------------------------------------------------------------%
%---------------------------- Conclusion -----------------------------%
%---------------------------------------------------------------------%
\chapter{Conclusion}

%---------------------------------------------------------------------%
%------------------------------ Appendix -----------------------------%
%---------------------------------------------------------------------%
\chapter*{Anhang}
\addcontentsline{toc}{chapter}{Anhang}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen
% Fortsetzung der römischen Seitennummerierung
\pagenumbering{Roman}
\setcounter{page}{\value{seitenzahlroemisch}}




\printbibliography



%\affidavit


\end{document}
