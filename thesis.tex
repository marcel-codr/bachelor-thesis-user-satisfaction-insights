%% This is file `DEMO-TUDaThesis.tex' version 2.09 (2020/03/13),
%% it is part of
%% TUDa-CI -- Corporate Design for TU Darmstadt
%%  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- -
%%
%%  Copyright (C) 2018--2020 by Marei Peischl <marei@peitex.de>
%%
%% ============================================================================
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2008/05/04 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainers of this work are
%%   Marei Peischl <tuda-ci@peitex.de>
%%   Markus Lazanowski <latex@ce.tu-darmstadt.de>
%%
%% The development respository can be found at
%% https://github.com/tudace/tuda_latex_templates
%% Please use the issue tracker for feedback!
%%
%% ============================================================================
%%
% !TeX program = lualatex
%%

\documentclass[
	english,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=1b,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=1mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
	DIV=14,
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}
\usepackage{datetime2}

% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
%\usepackage{iftex}
%\ifPDFTeX
%	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
%\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman, main=english]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}
\usepackage[super]{nth} % converts Dec \nth{1} to Dec 1st (with st as superscript)

%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage[style=authoryear-ibid,backend=biber]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\bibliography{references}
\AtBeginBibliography{\setcounter{maxnames}{50}} % Im Literaturverzeichnis nenne mehr als 2 Autoren
\counterwithout{footnote}{chapter} % Zähle Fußnoten über gesamtes Dokument hoch

%%%%%%%%%%%%%%%%%%%
% Code Styles
%%%%%%%%%%%%%%%%%%%
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegray}{rgb}{0.95,0.95,0.95}

\lstset{
  backgroundcolor=\color{codegray},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2,
  frame=single,
  rulecolor=\color{black},
  numbers=left,
  numberstyle=\tiny\color{gray}
}

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{seqsplit}
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
\usepackage{longtable} % Mehrseitige Tabellen
\usepackage{xtab,afterpage}
\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien
\usepackage{tikz}       % Graphen zeichnen
\usepackage{enumitem}   % for referencing in enumerate und description
\usepackage{amsmath}    % https://tex.stackexchange.com/questions/32140/how-to-write-a-function-piecewise-with-bracket-outside
% https://texblog.org/2012/03/21/cross-referencing-list-items/
\usepackage[acronym,shortcuts,nonumberlist,nomain]{glossaries} % for Abbreviations (Acronyms)
\usepackage{rotating}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{pdflscape}
%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

% Customs
%\usepackage[singlespacing]{setspace}
\usepackage[]{setspace}
\usepackage{caption}
%\usepackage[printonlyused]{acronym}

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}
\usepackage{hyperref}
\usepackage{xurl}

\makeglossaries{}

\begin{document}

\Metadata{
	title=Automatic evaluation of Enterprise-GenAI Applications success,
	author=Marcel Pfeiffer
}

\pagenumbering{gobble}

\title{Automatic evaluation of Enterprise-GenAI Applications success}

\studentID{2912332}
\author[M. Pfeiffer]{Marcel Pfeiffer}%optionales Argument ist die Signatur,
\birthplace{Koblenz}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Prof. Dr. Peter Buxmann}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
\department{wi} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Wirtschaftsinformatik}
\group{Software \& Digital Business}

\submissiondate{August 11, 2025}
\examdate{August 11, 2025}

% \tuprints{urn=1234,printid=12345}
%\dedication{Für alle, die \TeX{} nutzen.}

\maketitle
\newpage 

\begin{titlepage}
    % 1. Add a flexible space at the top to push everything down.
    % The number '1.5' can be increased to push the content further down.
    \vspace*{\stretch{2.0}}

    % 2. Your first block of content (student/thesis info)
    \noindent
    Marcel Pfeiffer \\
    Matriculation Number: 2912332 \\
    Course of study: Bachelor Wirtschaftsinformatik \\[0.75em]
    
    Bachelor thesis \\
    Title: "Automatic evaluation of Enterprise-GenAI Applications success" \\[0.75em]
    
    Date of submission: August 11, 2025 \\[0.75em]
    
    Supervisor: M.Sc. Adrian Glauben \\[0.75em]
    \noindent
    Prof. Dr. Peter Buxmann \\
    Wirtschaftsinformatik | Software \& Digital Business \\
    Law and Economics Department \\
    Technische Universität Darmstadt Hochschulstraße 1 \\
    64289 Darmstadt\\
    \noindent\rule{\textwidth}{0.4pt}
    
\end{titlepage}
\affidavit


% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  -- -- Abstract  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter*{Abstract}

GenAI, particularly RAG applications, is revolutionizing the business world. However, their success is not guaranteed and requires systematic evaluation for quality assurance and continuous improvement. Existing frameworks, such as RAGAS or ARES, often focus solely on \textit{Information Quality}, neglecting other key success factors.

This bachelor's thesis addresses this gap by conceptualizing and implementing an analytics dashboard for the automatic and holistic measurement of an enterprise RAG application's success. Based on the IS Success Model by DeLone and McLean, a dashboard is integrated into an enterprise RAG system to measure and display metrics for the three central quality dimensions: \textit{Information Quality}, \textit{System Quality} and \textit{Service Quality}. The dashboard was developed using the DSR methodology in close collaboration with the development team of the RAG software.

Core features include the AI-supported evaluation of answer and document quality, the visualization of topics in user requests and the analysis of the most important task types.

An evaluation with the development team confirmed the dashboard's high practical value. The additional analytics functions add an extra cost of approximately 20\% relative to existing LLM costs, which is justified by the added value. In summary, this work establishes a validated foundation for the data-driven, holistic success measurement of GenAI systems in enterprises.
\tableofcontents


\pagenumbering{Roman}

% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  -- -- List of Figures  --  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %

\listoffigures
\addcontentsline{toc}{chapter}{List of figures}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  List of Tables  --  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %

%\listoftables
%\addcontentsline{toc}{chapter}{List of tables}           %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen

% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  -- -- List of Abbreviations  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter*{List of abbreviations}
\addcontentsline{toc}{chapter}{List of abbreviations}            %Kapitel bekommt keine Nummerierung und wird trotzdem
                                                                %ins Inhaltsverzeichnis aufgenommen

\begin{xtabular}{ll}
AI & Artificial Intelligence\\
API & Application Programming Interface\\
ARES & Automatic RAG Evaluation and Scoring \\
DSR & Design Science Research\\
GCP & Google Cloud Platform\\
GDP & Gross Domestic Product\\
GDPR & General Data Protection Regulation\\
GenAI & Generative AI\\
HDBSCAN & Hierarchical Density-Based Spatial Clustering of Applications with Noise\\
HIPAA & Health Insurance Portability and Accountability Act\\
HTTP & Hypertext Transfer Protocol\\
IS & Information Systems\\
ISSM & Information Systems Success Model\\
KM & Knowledge Management\\
KMS & Knowledge Management System\\
KPI & Key Performance Indicator\\
LLM & Large Language Model\\
ML & Machine Learning\\
PEU & Perceived Ease of Use\\
PU & Perceived Usefulness\\
RAG & Retrieval-Augmented Generation\\
RAGAS & Retrieval-Augmented Generation Assessment \\
TAM & Technology Acceptance Model\\
t-SNE & t-distributed Stochastic Neighbor Embedding \\
UMAP & Uniform Manifold Approximation and Projection\\
\end{xtabular}
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  -- - Introduction  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\cleardoublepage 
\onehalfspacing
% Umstellen auf arabische Seitenzahlen. Die Anzahl der bisherigen
% Seiten wird zwischengespeichert, da die römische Nummerierung am
% Ende des Dokuments fortgesetzt werden soll.
\newcounter{seitenzahlroemisch}
\setcounter{seitenzahlroemisch}{\value{page}}
\pagenumbering{arabic}
\chapter{Introduction}
ChatGPT reached 1 million users within 5 days after its launch in 2022, which was only exceeded by Threads in 2023 \parencite{Brandt2023}. Today, ChatGPT benefits 800 million weekly users \parencite{Huber2025} and has thus become an established Generative Artificial Intelligence (GenAI) tool. GenAI bears high potential not only for the private sector but also for the modern business world. Projections underscore this, suggesting that GenAI could increase Germany's GDP by up to €585 billion by 2040 \parencite[p.~12]{McKinsey2023}. Furthermore, for enterprise applications, Retrieval-Augmented Generation (RAG) has emerged as a particularly suitable technology by connecting Large Language Models (LLMs) to company-specific proprietary knowledge bases  \parencite[p.~9459]{Lewis2020}. Connecting and bounding LLMs to proprietary knowledge bases not only enables them to access up-to-date company data \parencite{GoogleCloudRAG} but also enhances transparency and reduces the likelihood of hallucinations, as the company data serves as the ground truth  \parencite[p.~9469]{Lewis2020}.

However, the success of enterprise RAG applications typically requires some effort, including the use of specialized RAG pipelines or connecting various APIs to the RAG system, to implement them effectively \parencite[pp.~5--7]{Bruckhaus2024RAG}. Common failure points include the system failing to provide an answer even though the information is available, generating a false answer by misinterpreting context or returning an incomplete response \parencite[p.~91]{Barnett2024}. As organizations invest in and deploy these powerful but complex systems, a critical management challenge arises: How can the success of a GenAI application be measured and monitored over time? Without systematic evaluation, system administrators lack the data-driven insights needed to identify weaknesses, justify improvements and ensure that the system delivers a positive return on investment.

This thesis addresses this management challenge directly. It documents the design, development and evaluation of an analytics dashboard created to measure the success of an enterprise GenAI application automatically. Following the Information Systems Success Model, success can be measured in the three quality dimensions: \textit{Information Quality}, \textit{System Quality} and \textit{Service Quality} \parencite[pp.~23--24]{DeloneMcLean2003ISSuccessTenYearUpdate}. However, most RAG evaluation frameworks like RAGAS \parencite{Es2024} or ARES \parencite{Saad-Falcon2024} only include the \textit{Information Quality} provided by the system. To move beyond this narrow perspective on IS Success, this study employs a Design Science Research methodology following \cite{Peffers2007} to conceptualize and implement a dashboard based on a more holistic perspective.
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  Theoretical Background  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter{Theoretical Background}
In order to gain a deeper understanding of what determines the success of an enterprise GenAI system, we will first look at the success of Information Systems (IS) in general. Afterwards, we will dive deeper into the context of Knowledge Management Systems -- specifically enterprise GenAI systems and apply the more general theories to them. This will help us define and categorize the software features we are going to implement during our artifact development.
\section{Technology Acceptance and IS Success}
To better understand the success of IS, we will have a look at two models. The Technology Acceptance Model (TAM) \parencite{Davis1989} deals with the question, which determinants lead to the acceptance and use of an IS by a single user. In contrast, the IS Success Model \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} (ISSM) focuses on the success of an IS in an organizational context.
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{images/TAM.png}
    \caption{Technology Acceptance Model by \parencite{Davis1989} as in \parencite[p.~61]{Ma2004}}
    \label{fig:enter-label}
\end{figure}
\\
Looking at the TAM, two determinants influence the use of an IS - the \textit{Perceived Usefulness} (PU) and the \textit{Perceived Ease of Use} (PEU) \parencite[p.~320]{Davis1989}. Both are influenced by \textit{External Variables} \parencite[p.~335]{Davis1989}. The PU describes the degree to which a person believes that using a particular system would be beneficial to them \parencite[p.~320]{Davis1989}. In an enterprise context, this usually means the conviction that the use of the IS has a beneficial influence on individual job performance. The second determinant, the \textit{Perceived Ease of Use} (PEU), refers to "the degree to which a person believes that using a particular system would be free of effort" \parencite[p.~320]{Davis1989}. This idea relates to the user's estimation of the mental effort required to learn and operate the system. If a system is intuitive, straightforward and does not require extensive training to use, it will be perceived as easy to use \parencite[pp.~324-325]{Davis1989}.\\
A crucial aspect of the TAM is the relationship between these two determinants. Davis argues that the PEU has a direct, positive influence on the PU \parencite[p.~334]{Davis1989}. A system that is easier to use is also perceived as more useful, partly because the user can invest less effort in the mechanics of using the system and more in the actual task at hand. Both PU and PEU have been found to be significantly correlated with \textit{Intentions to Use} and \textit{actual System Use} \parencite[p.~333]{Davis1989}. However, it is important to note that a consistent finding in TAM research is that PU has a significantly stronger influence on \textit{Usage Intention} and \textit{actual Use} than PEU \parencite[p.~333]{Davis1989}. While ease of use is important, the ultimate driver for acceptance is the user's belief that the system will help them perform their job better.\\
\\
Now that we know which factors lead to the use of an IS by a single person, we will shift our perspective to the factors that determine its use and overall success within an organizational context.
Following \cite{DeloneMcLean2003ISSuccessTenYearUpdate}, six interrelated factors determine the success of an IS in an organizational context: \textit{System Quality, Information Quality, Service Quality, Intention to Use} and actual system \textit{Use}, \textit{User Satisfaction} and the \textit{Net Benefits} of the IS.\\
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{images/ISSuccess.png}
    \caption{IS Success Model \parencite[p.~24]{DeloneMcLean2003ISSuccessTenYearUpdate}}
    \label{fig:enter-label}
\end{figure}
\\
\textit{System Quality} means the quality of the IS in terms of technical performance \parencite[p.~64]{DeloneMcLean2003ISSuccessTenYearUpdate}. Typical measures for \textit{System Quality} would be, for example, response time, reliability, ease of use, or hardware utilization \parencite[p.~64--65]{DeloneMcLean1992ISSuccess}.\\
\textit{Information Quality} refers to the quality of the information provided by the IS \parencite[p.~64]{DeloneMcLean1992ISSuccess}. This includes, for example, the usefulness, timeliness, precision, completeness and relevance of the information provided by the system \parencite[p.~67]{DeloneMcLean1992ISSuccess}.\\
\textit{Service Quality} means the quality of the support that users receive from the IS department or service provider \parencite[p.~18]{DeloneMcLean2003ISSuccessTenYearUpdate}. It became an important success dimension with the rise of end-user computing, where IS providers needed to provide services in addition to making them usable. This dimension is often measured by the support staff's responsiveness, assurance and empathy.
\\
These three aspects of the quality of an IS directly influence the \textit{Intention to Use} and the system \textit{Use} as well as the \textit{User Satisfaction} \parencite[p.~23--24]{DeloneMcLean2003ISSuccessTenYearUpdate}. Here, it is important to differentiate between the two concepts of \textit{Use}. \textit{Intention to Use} is an attitudinal measure, representing a user's plan to engage with a system \parencite[p.~23]{DeloneMcLean2003ISSuccessTenYearUpdate}. In contrast, system \textit{Use} is a behavioral measure of the actual interaction, captured by metrics like frequency, duration, or the amount of advanced features used \parencite[p.~66]{DeloneMcLean1992ISSuccess}. This distinction is particularly crucial in an enterprise context. For a system where \textit{Use} is voluntary, a user's intention is a strong predictor of their actual \textit{Use}. However, if a system's use is mandatory, simply measuring that it is being used is not very meaningful, because the system has to be used anyway. In that scenario, assessing the nature of the \textit{Use} (e.g., are users leveraging advanced features?) or their underlying \textit{Intention to Use} the system properly becomes a more telling indicator of true acceptance and success
\parencite[p.~66]{DeloneMcLean1992ISSuccess}. Both \textit{Use} and \textit{Intention to Use} are linked with \textit{User Satisfaction}. A positive experience during system \textit{Use} leads to higher \textit{User Satisfaction}, which in turn reinforces the intention for continued and more extensive \textit{Use} in the future \parencite[p.~23]{DeloneMcLean2003ISSuccessTenYearUpdate}. In an organizational context, this \textit{User Satisfaction} is heavily influenced by the system's perceived impact on job performance, such as an increase in productivity. Therefore, the combination of active system \textit{Use} and high \textit{User Satisfaction} is what generates \textit{Net Benefits} \parencite[p.~23]{DeloneMcLean2003ISSuccessTenYearUpdate}.\\
This process, however, is not a one-way street. The model includes a crucial feedback loop: the \textit{Net Benefits} that users perceive will in turn influence their future \textit{User Satisfaction} and \textit{Intention to Use} \parencite[p.~23-24]{DeloneMcLean2003ISSuccessTenYearUpdate}. When users see that a system is delivering real value, it reinforces their positive attitude towards the system. This creates a virtuous cycle, where a successful system's value is constantly reinforced and grows over time \parencite[p.~23]{DeloneMcLean2003ISSuccessTenYearUpdate}.\\
\\
The TAM and the ISSM address the same fundamental questions but from two different perspectives. The TAM focuses on the individual user, suggesting that a person will use a system more often if it provides value (PU) and is easy to operate (PEU) \parencite[p.~320]{Davis1989}.
The ISSM, on the other hand, argues from a company's perspective. It concludes that three quality dimensions -- \textit{Information Quality}, \textit{System Quality} and \textit{Service Quality} -- influence system success. These dimensions impact both \textit{User Satisfaction} and \textit{Intention to Use}, which drives actual system \textit{Use} \parencite[pp.~22-23]{DeloneMcLean2003ISSuccessTenYearUpdate}. Overall, the two models appear to be quite similar to each other, as both incorporate factors such as the simplicity of a system, its overall benefit and the nature of the \textit{Use}. Therefore, we can conclude that evaluations conducted on a small individual scale will also be valid on an organizational scale.

For this thesis, the ISSM provides a more practical framework. It offers specific, measurable dimensions rather than the TAM's general concept of \textit{External Variables} and argues from a company's perspective. Therefore, we will use the ISSM to develop and categorize the quality measurements for the RAG application.
\newpage
\section{Knowledge Management System's Success}
Now that we have a solid understanding of IS Acceptance and Success in general, we will take a closer look at Knowledge Management Systems (KMS). We'll start with some definitions that will help us understand what knowledge, Knowledge Management (KM) and KMS are. "Knowledge is a fluid mix of framed experience, values, contextual information and expert insight that provides a framework for
evaluating and incorporating new experiences and information" \parencite[p.~4]{Davenport1998}. Knowledge in terms of organizational memory is a key factor in decision-making processes in organizations \parencite[p.~52]{Jennex2006}. KM, on the other hand, "... is the practice of selectively applying knowledge from previous experiences of decision making to current and future decision making activities with the express purpose of improving the organization’s effectiveness" \parencite[p.~1]{Jennex2006WhatIsKM}. KMS in the end can be defined as "IT-based systems developed to support and
enhance the organizational processes of knowledge creation, storage/retrieval, transfer and application" \parencite[p.~114]{Alavi2001}. Overall, we can conclude that KMS are systems that support the decision-making process, e.g., in a company by managing and selectively providing knowledge to their users. Since KMS are by definition a specialized form of IS we can simply apply the ISSM to them.\\
Following \cite{Jennex2006}, we can extend the three quality factors to gain a better understanding
of KM success.\\
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/knowledge_management_success.png}
    \caption{Knowledge Management Success Model by Jennex \& Olfman \parencite[p.~56]{Jennex2006}}
    \label{fig:enter-label}
\end{figure}
The \textit{System Quality} dimension extends to contain the \textit{Technological Resources}, \textit{KM Form} and the \textit{KM Level} \parencite[p.~56]{Jennex2006}. \textit{Technological Resources} define the capability of an organization to develop, operate and maintain KMS \parencite[p.~56]{Jennex2006}. This includes all kinds of technical conditions like the amount of experience available to develop, operate and maintain the KM, the underlying network infrastructure or the databases to hold the data used in KM. The \textit{KM Form} refers to the extent to which knowledge and KM processes are computerized and integrated \parencite[p.~56]{Jennex2006}. This is important since computerized KM processes are more effective than analogue ones as they make knowledge accessible everywhere at any time through a simple-to-use interface. The \textit{KM Level} in the end refers to the ability to influence current activities \parencite[p.~57]{Jennex2006}. \textit{Technological Resources} and the \textit{KM Form} are both enablers for the \textit{KM Level} and serve as a technical foundation for the ability of the KMS to influence the decision-making process.\\
The \textit{Knowledge Quality} dimension, that corresponds to the \textit{Information Quality} dimension in the ISSM \parencite[~p.~64]{DeloneMcLean2003ISSuccessTenYearUpdate}, consists of three subsidiary factors too - the \textit{KM Strategy/Process}, the \textit{Richness} of the captured knowledge and the \textit{Linkages} among the knowledge components \parencite[p.~57]{Jennex2006}. The \textit{KM Strategy/Process} factor is the basis for the two other factors and looks at the organizational processes organizing the KM and to what extent these processes are formalized \parencite[p.~57]{Jennex2006}. \textit{Richness}, on the other hand, evaluates the accuracy and timeliness of the knowledge itself - one could say it is the quality of the actual knowledge \parencite[p.~57]{Jennex2006}. \textit{Linkages} refer to how accessible the knowledge is through things like knowledge and Topic Maps, etc. \parencite[p.~57]{Jennex2006}\\
The \textit{Service Quality} dimension determines if users of the KMS are provided with sufficient support to use the KMS effectively \parencite[p.~58]{Jennex2006}. It consists of three parts as well. The \textit{Management Support} refers to the provision of adequate resources for the KMS of the organization as well as the creation of a knowledge-sharing organization culture by the organization's management \parencite[p.~58]{Jennex2006}. \textit{User KM Service Quality} refers to the support provided by the user organization in order to train the users to use the KMS effectively \parencite[p.~58]{Jennex2006}. On the other hand, the \textit{IS KM Service Quality} refers to the support provided by the IS organization to allow users to use the KMS as well as maintaining them \parencite[p.~59]{Jennex2006}. This includes building, setup and maintaining the KMS infrastructure, ensuring availability, reliability and security of the KMS provided.\\\\
All three dimensions - the \textit{System Quality}, \textit{Knowledge Quality} and the \textit{Service Quality} influence similar to the original ISSM the \textit{Intention to Use} as well as the \textit{User Satisfaction} and cause a \textit{Net Benefit} in the end \parencite[p.~56]{Jennex2006}. The KMS Success model makes a small difference by replacing the \textit{Usage} with \textit{Perceived Benefit} to highlight the importance of the distinction between mandatory and voluntary \textit{Use} \parencite[p.~55]{Jennex2006}.
\section{GenAI Systems in Organizations}
Since we now know what determines the success of KMS in organizations like enterprises, we are going to have a closer look at the GenAI systems that are used to support KM in an organization, find out what they are used for and which factors define their quality.\\
Artificial Intelligence (AI) in general is hard to define. For now, we will follow the definition by Marvin Minsky and define AI as "the science of making machines do things that would require intelligence if done by men" \parencite{Minsky1966} as cited in \parencite{Dennis2025}. In other words, AI systems are systems that show some form of intelligent behavior similar to human intelligence. Machine Learning (ML) is a subfield of AI and describes the analysis of huge amounts of data \parencite[p.~573]{Lund2023}. Neural networks are models used in ML to perform this analysis. They imitate the human brain by using interconnected layers of artificial neurons (or nodes) to process information \parencite[p.~573]{Lund2023}. Generative AI (GenAI) is one kind of ML model approach that uses neural networks with a vast number of layers to generate new data based on training data \parencite[p.~574]{Lund2023}.
Large Language Models (LLMs) are a type of GenAI model. These models are trained on huge amounts of data and are capable of many text-processing tasks \parencite[p.~2]{Naveed2024}. The workflow using an LLM is as follows: A user submits a request, known as a prompt, to the LLM and the LLM generates a response based on its training data and the prompt \parencite[p.~575]{Lund2023}.\\
The GenAI system we will focus on throughout this thesis is a Retrieval-Augmented Generation (RAG) system. These systems utilize external data sources like web pages, other organizational KMS, or third-party sources through retrieval algorithms \parencite{AWS_RAG}. The retrieved context is then passed alongside the user's query to an LLM that generates an answer grounded on the retrieved knowledge. Such RAG systems can be used for open-domain question answering and for knowledge work in an organization as they make (proprietary) knowledge available to an LLM \parencite[p.~9459]{Lewis2020}.
\subsection{Advantages of RAG Systems for organizational KM}
One problem of LLMs that do not use RAG is that they can only provide the data they were trained on. This causes them to become outdated. As we know from the KMS Success Model, an organization must be able to access up-to-date information using their KMS. RAG systems can provide this by utilizing up-to-date knowledge sources for their generation \parencite[p.~9459]{Lewis2020}.\\
Another issue with LLMs is that they tend to hallucinate if they don't know the answer to a user's query or apply a pattern from the training data incorrectly. Since RAG systems bind the LLMs they use to the information provided by the retrieval algorithms, they reduce hallucinations, allowing a user to verify the answer provided by the LLM by double-checking the sources used by the RAG system \parencite[p.~9469]{Lewis2020}. This way, the RAG approach improves the accuracy of LLMs.\\
Since RAG systems can query various data sources at once \parencite{GoogleCloudRAG}, they provide good \textit{Linkages} to other KMS as well. Following the IS \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} and KMS Success Models \parencite{Jennex2006}, RAG systems appear to be a suitable choice for organizational KM.
\subsection{Challenges for RAG Systems in Organizations}
However, realizing the full potential of RAG systems in an enterprise context requires significant effort. Enterprises typically have high requirements concerning not only the output quality but also compliance regulations, the integration of various data sources and other factors. In the following analysis, we will examine some of the issues encountered in RAG systems used in enterprises and assign them to the quality dimensions of the ISSM. \\
A formal taxonomy of failure points for a RAG system, created based on three case studies, highlights seven distinct issue types related to \textit{Information Quality} \parencite[p.~91]{Barnett2024}. These challenges range from fundamental data gaps, where the required information is simply missing from the knowledge base, to retrieval failures where required documents are not ranked highly enough by the retrieval algorithm or are lost during context consolidation. Further down the pipeline, the LLM might fail to extract the correct answer from the provided context. Finally, even if factually correct, the generated answer may have the wrong format, an incorrect level of specificity, or be incomplete, failing to fully satisfy the user's query.\\
Three other case studies demonstrate these theoretical \textit{Information Quality} related issues in the real world too. In a 2023 case study, a RAG system implemented in a major healthcare organization began to produce a series of potentially harmful treatment suggestions that contradicted current medical standards. This occurred because its knowledge base contained outdated medical guidelines, inconsistent drug interaction databases and incomplete patient history records \parencite{NStarX_Inc_2025}. This case highlighted the direct risks to patient safety. In another example, an investment firm's RAG system provided contradictory advice on securities regulations, potentially leading to compliance violations. An investigation found the knowledge base contained regulatory documents from multiple jurisdictions without proper geographic tagging, superseded regulations that had not been archived and inconsistent interpretations of the same rule. The firm faced potential regulatory scrutiny and was forced to suspend the system, resulting in millions of dollars in lost productivity and increased manual oversight \parencite{NStarX_Inc_2025}. Similarly, a telecommunications chatbot began providing customers with incorrect billing information and outdated service plans. The root cause was traced to duplicate customer records, outdated product catalogs with discontinued services and inconsistent pricing data across regional databases \parencite{NStarX_Inc_2025}.\\
Another study highlights five key challenges that RAG systems face, especially in compliance-regulated enterprise environments \parencite[p.~6]{Bruckhaus2024RAG}.\\
The first of these is maintaining accuracy, consistency and explainability \parencite[p.~6]{Bruckhaus2024RAG}, which is related to \textit{Information Quality} \parencite[pp.~64--66]{DeloneMcLean1992ISSuccess}. Given that RAG outputs in sectors such as finance or healthcare can carry legal or financial weight, they require a higher level of trustworthiness and accuracy. Furthermore, the systems must be able to explain how retrieved documents influence the final generated content and provide confidence scores to help users assess the reliability of the output.\\
Secondly, data security, privacy and compliance are critical \parencite[p.~6]{Bruckhaus2024RAG}. Enterprises must ensure their RAG systems comply with strict regulations, such as HIPAA or GDPR, which require robust data encryption, access controls and detailed audit trails to prevent the unauthorized disclosure of sensitive information.\\
A third significant \textit{System Quality}-related \parencite[p.~64]{DeloneMcLean1992ISSuccess} challenge is the scalability and performance of the RAG application \parencite[p.~6]{Bruckhaus2024RAG}. Enterprise knowledge bases are typically vast and complex, spanning numerous formats and systems. RAG architectures must be able to efficiently index, update and search these massive data sources while maintaining high performance and low latency.\\
Fourth, integration and interoperability with existing IT infrastructure are essential \parencite[p.~6]{Bruckhaus2024RAG}. RAG systems must be designed to seamlessly integrate with an organization's content management platforms and databases, often requiring flexible APIs and custom connectors. As these requirements require effort by the IS team of the RAG application, they relate to the \textit{IS System Quality} \parencite[pp.~58--59]{Jennex2006}.\\
Lastly, customization and domain adaptation are crucial for achieving high accuracy \parencite[p.~6]{Bruckhaus2024RAG}. Each enterprise uses a unique data schema, taxonomy and domain-specific language. RAG systems must be adaptable, allowing for the fine-tuning of language models and retrieval algorithms to understand this specialized context and improve the relevance and coherence of the generated results. While the accuracy itself relates to \textit{Information Quality} \parencite[pp.~64--66]{DeloneMcLean1992ISSuccess}, adopting the RAG system to handle domain-specific requirements requires a high \textit{Service Quality} \parencite[p.~17]{DeloneMcLean2003ISSuccessTenYearUpdate} since the teams supporting the system have to perform customizations.\\
The study concludes that RAG systems need to be purpose-built, as general-purpose RAG systems cannot adequately handle these challenges \parencite[pp.~6--7]{Bruckhaus2024RAG}.\\
In conclusion, while RAG systems offer immense potential for enhancing an organization's KM, considerable effort is required to realize this potential. Since the issues affect all three quality dimensions of the ISSM, an ideal evaluation framework should provide evaluation features that consider all three of them.
\section{Evaluation and Analytics Frameworks}
Several frameworks already exist for evaluating the \textit{Information Quality} of a RAG system. A prominent open-source example is the RAGAS evaluation framework, which in its original version uses an "LLM as a judge" approach to determine quality metrics \parencite[pp.~123--124]{Es2024}. Here, answer relevance assesses whether the answer fits the question, faithfulness evaluates whether the retrieved documents support the answer, and context relevance measures whether the retrieved documents are relevant to the question. The framework has since been expanded to include a broader range of metrics, including some that are not model-based \parencite{RagasMetrics}; however, a complete list is beyond the scope of this section. Other open-source solutions, such as ARES, also rely on LLM-based evaluation models \parencite[p.~216]{Saad-Falcon2024}.

Proprietary services also exist, such as Google's Gen AI Evaluation Service available on the Google Cloud Platform, which uses a mix of LLM-based and computation-based metrics to assess RAG quality \parencite{GoogleGenAIEvaluation}. This shows that using an LLM to evaluate RAG quality is a common and established practice, an approach that is also adopted in the evaluation dashboard developed in this thesis.

However, the frameworks mentioned above focus exclusively on RAG quality, which corresponds solely to the \textit{Information Quality} dimension of the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}. This thesis aims to cover the other quality dimensions as well. Existing systems often achieve a broader analysis through the integration of general analytic tools, e.g., Google Analytics \parencite{GoogleAnalytics}. Such tools enable the tracking of detailed user interactions, providing a more comprehensive picture of system usage. For instance, \textit{System Quality} \parencite[p.~64]{DeloneMcLean1992ISSuccess} can be assessed by measuring end-to-end page load times or errors to evaluate performance and reliability. \textit{Service Quality} \parencite[p.~18]{DeloneMcLean2003ISSuccessTenYearUpdate} can be analyzed by tracking feature usage and monitoring engagement with advanced features to understand how users interact with the system.

The goal of this thesis is to develop a dashboard that combines both approaches. It aims to move beyond a narrow focus on RAG-specific \textit{Information Quality} to provide a foundation for a holistic analysis of all quality dimensions of the ISSM.\\
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  Methodology  --  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter{Methodology}
The research methodology is based on the Design Science Research (DSR) approach following \cite{Peffers2007}, which involves the definition, development and evaluation of artifacts in iterative cycles. DSR aims to develop innovative solutions for specific problems and to test these solutions in real-world contexts.
\section{Problem Identification and Motivation}
In the first phase of the DSR approach, the need for support in evaluating the RAG system using a holistic approach following the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} was identified. System administrators require insights that enable them to assess the current performance and quality of the RAG application more effectively and draw conclusions about the measures that must be taken to improve it. Examples of such measures include employee training, enhancing the knowledge base, or improving the RAG mechanism itself.
\section{First Iteration - Definition of valuable Insights}
In the first iteration, specific software features were defined to support system administrators in managing the system. The software features were ideated in collaboration with the developers of the investigated RAG software during several joint workshops.
\subsection{Define Objectives of Artifacts}
In the first workshop, the developers collected initial ideas for evaluating the RAG application. They analyzed its current data situation to determine what additional data needed to be gathered to implement the automated analysis. The foundational principles of IS \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} and KMS Success \parencite{Jennex2006} were presented to the developers. They then developed ideas on how to evaluate the RAG application and which data needs to be collected in small groups and afterwards discussed their findings in a plenary session. Through this process, the objectives for the evaluation of the RAG system were established
\subsection{Development of Artifacts}
Based on the developers' ideas, specific concepts for software features were developed and then classified into the quality categories of the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}.
\subsection{Demonstration and Evaluation}
In a second workshop, the developed concepts were presented to the developers and feedback on them was gathered. Additionally, the developers prioritized the individual software features with the categories low, medium and high to ensure that the subsequent implementation phase could focus on developing and delivering the most important ones.
\section{Second Iteration - Implementation of Features}
In the second iteration, the developed software concepts were technically implemented and integrated into the existing RAG application.
\subsection{Define Objectives of Artifacts}
First, different technical implementation solutions were developed and presented to the development team once again. Feedback on them was gathered and a decision was made on how the features should be technically implemented.
\subsection{Development of Artifacts}
Subsequently, the software features were implemented and integrated into the existing RAG application. Technical reviews by the developers were conducted to validate the correctness of the software features created.
\subsection{Demonstration and Evaluation}
Finally, the software features created were presented to the development team in another workshop and feedback on them was gathered.
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  Results  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter{Results}
In the following chapter, we will elaborate on the concepts for software features developed during the workshops and how they have been implemented in the RAG application. Feedback from the developer team and their prioritization of features will also be discussed.
\section{Description of the RAG System used in the Study}
To provide a basis for further investigation, this section describes the features of the RAG system under examination. The system is designed for enterprise use, making company-specific knowledge accessible to employees through a conversational interface powered by LLMs.
\subsection{System Architecture and Technology Stack}
The system is deployed on the Google Cloud Platform (GCP) \parencite{GoogleCloudDocumentation} and utilizes Vertex AI Search \parencite{GoogleVertexAISearch} for the initial document retrieval from its knowledge base. The relevance of the retrieved documents is subsequently optimized by the Vertex AI Reranker \parencite{GoogleReranker}. For the generation component, the system is capable of integrating various LLMs through the LangChain framework \parencite{LangChain}. Given its deployment on GCP, Google's Gemini models \parencite{GoogleGemini} are typically employed for response generation.\\
A central architectural feature of the system is the implementation of distinct Use Cases, which are customized units capable of handling requests to a specified set of data. The customization allows the system to provide context-aware responses by augmenting the generation prompt with use-case-specific information, such as a specialized glossary or other supplementary data like a use-case-specific tonality. Each Use Case is associated with a specific selection of so-called Knowledge Assets, which are custom compilations of so-called Knowledge Sources. A Knowledge Source contains information on a particular topic and is technically linked to a Datastore in Vertex AI Search, which performs the underlying retrieval and holds the specific data.\\
The user requests are organized as follows: A user can start a Thread within a Use Case. This Thread can contain multiple Chats that are linked to it. The user's requests are connected to a Chat in turn.\\
A Use Case and the data used in a Use Case are managed by a so-called Use Case admin, who is a trained employee of the company using the RAG system. The Use Case admins are the actual target group of the created dashboard. They are supposed to use the information presented to them to enhance the RAG system in the end.
\subsection{Retrieval and Query Enhancement Features}
The system incorporates several advanced features to enhance the retrieval process:
\begin{itemize}
    \item \textbf{Query Splitting:} The user's search query is semantically decomposed into multiple sub-queries. A separate retrieval process is executed for each sub-query to ensure every semantic aspect of the user's query is covered \parencite[p.~8]{Ammann2025}.
    
    \item \textbf{Query Reformulation:} The system supports the reformulation of user queries to add necessary context, e.g., from previous questions \parencite[pp.~6--7]{Setty2024}.
    
    \item \textbf{Metadata Filtering:} The search performed by Vertex AI Search can be refined by applying a metadata filter. This allows for the retrieval to be constrained to only those documents that match specific attributes \parencite[pp.~7--8]{Setty2024}.
\end{itemize}
\subsection{User Interface and Interaction Features}
The system provides a range of interactive features to enhance the user experience and workflow integration:
\begin{itemize}
    \item \textbf{Question Suggestions:} After an answer is generated, the system proposes alternative and potential follow-up questions to the user's initial question that the user can select with a single click.
    
    \item \textbf{Document Viewer:} The documents retrieved during the search process can be viewed in an integrated file viewer and are available for download too.
    
    \item \textbf{Feedback:} Users can provide individual feedback on their satisfaction with the LLM responses.

    \item \textbf{Copy Function:} Users can copy the answer provided by the LLM using a Copy Button
    
    \item \textbf{Quick Actions:} The system offers "Quick Actions," which are clickable shortcuts that allow users to perform subsequent tasks with the generated information, such as creating an email or generating a summary.
\end{itemize}
\section{Software features}
This chapter presents the results of the developer workshops, including the resulting concepts and their relationship to the IS \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} and KMS Success \parencite{Jennex2006} frameworks, as well as the prioritization and feedback provided by the developer team. Each concept is explained in terms of the quality dimensions that it covers. While many concepts allow Use Case administrators to improve the system across multiple quality dimensions, they are assigned to the dimension for which they are most suitable.
\subsection{Mostly System Quality related Concepts}
\subsubsection{Response Time Measurement}
The first concept for determining the \textit{System Quality} of the RAG application is the measurement of its response time, defined as the duration from the submission of a user's query to the delivery of the complete answer. For a comprehensive analysis, this measurement should incorporate not only the raw duration but also key system factors to assess their impact. These factors include the specific language model (LLM) being used and whether features such as Query Splitting or Query Reformulation were activated for the request. Using this feature, Use Case administrators can identify performance trends and retrospectively evaluate the impact of their own decisions on the response times of the RAG system. Furthermore, it enables them to better assess how various system factors affect the response time. These insights can, in turn, be used to train employees on the effective use of the system, for example, by providing better guidance on the performance implications of choosing a particular language model. By evaluating the response time of the system, Use Case administrators can draw conclusions on the performance of the system and whether the \textit{Technological Resources} provided are suitable for the system, which is a key aspect of \textit{System Quality} \parencite[p.~56--57]{Jennex2006}.

For the visualization of this data, a time-series diagram is proposed. The x-axis would represent the time and the y-axis the measured response time. To improve readability and highlight long-term developments, the chart should be smoothed, for instance, by displaying daily averages. The interface should also include interactive filters that allow for the inclusion of the different system factors. These filters would enable users to display data subsets, such as showing only the response times associated with a specific LLM or with Query Splitting enabled/disabled.

During the workshop, the development team assigned a \textbf{medium priority} to this feature (see Tables \ref{tab:system_quality}, \ref{tab:system_quality_g2} in the appendix). In their feedback, they noted that while the feature would be straightforward to implement, they suspected that overall response times would not vary significantly. Therefore, they hypothesized that this analysis would likely not yield major conclusions.
\subsubsection{Error Rate Monitoring}
A second concept proposed to evaluate \textit{System Quality} focuses on monitoring error rates to assess system stability and reliability. The core metrics to be captured are the absolute count and the percentage of user queries that result in a system error. For a more granular analysis, these error metrics can be combined with the system factors (LLM selection and the activation state of Query Splitting and Reformulation) and with the different predefined error types and messages of the system. By analyzing the error rates of the system, Use Case administrators can conclude the overall reliability of the system, which is yet another critical \textit{System Quality} metric \parencite[p.~64]{DeloneMcLean1992ISSuccess}. By providing the development and operations team of the RAG application with insights into frequent errors, this feature partly contributes to the \textit{IS KM Service Quality} \parencite[pp.~58--59]{Jennex2006} of the RAG system, as well, since service processes carried out by the RAG system's vendor, such as bug fixes, are enhanced.

The visualization proposed for these data is again a time-series diagram. The x-axis would represent time, while the y-axis would display the amount or percentage of system errors. The interface is designed to be interactive, featuring filtering options for both system factors and error types. This allows for detailed analysis, such as comparing the error rates of different language models or analyzing the impact of query splitting and query reformulation on error rates.

The workshop feedback showed different opinions on this feature's usefulness and priority. One group assigned it a \textbf{high priority} (see Table \ref{tab:system_quality} in the appendix). They argued that system errors are a major problem that can stop people from using the application and cause them to lose interest. This perspective highlights the feature's role in keeping users engaged. A second group assigned it a \textbf{medium priority} (see Table \ref{tab:system_quality_g2} in the appendix), seeing it mainly as a diagnostic tool for developers. For them, the main benefit was helping the development team find and fix technical issues faster. This group strongly recommended that this monitoring view should be for internal use by the development team only. They reasoned that showing raw error statistics to Use Case administrators or end-users could damage their trust in the system, which is not in the interest of the RAG systems vendors. In the end, the feature was assigned a \textbf{medium priority} since the arguments from the critical perspective predominated the other ones.
\subsection{Mostly Information Quality related Concepts}
\subsubsection{Topic Map}
To better understand what information users are looking for, the concept of a "Topic Map" was developed. This feature is designed to visualize the main topics that appear in the RAG system. The map can display topics from three different data sources: the user queries themselves, the content of the documents retrieved by the system, or the knowledge assets that were accessed. By making the most important topics visible to the Use Case admins, the \textit{Information Quality} and following the KMS Success framework \textit{Richness} and \textit{Linkages} \parencite[pp.~57--58]{Jennex2006} can be enhanced, e.g., by optimizing the underlying data for these topics. \textit{User KM Service Quality} \parencite[pp.~58--59]{Jennex2006} can be improved as well, since the Use Case administrator can analyze whether users are choosing a suitable Use Case for their requests and provide guidance if needed.

In the idea discussed in the workshop, the map is displayed as a bubble plot, where each bubble represents a specific topic. The positioning is semantic, so that topics with similar meanings are positioned closer to each other and related topics are grouped by color. The size of each bubble shows how frequent or prominent a topic is. The feature also includes a time filter, which allows administrators to see how user interests and queried topics change over a specific period.

The development team rated the overall Topic Map feature as a \textbf{high priority} (see Tables  \ref{tab:info_quality_p1}, \ref{tab:info_quality_g2_p1} in the appendix), but they assigned different levels of importance to analyzing each of the three data sources:

\begin{itemize}
    \item \textbf{Topics from user queries:} This received a \textbf{high priority}. The team agreed that this view would provide the most valuable insights into what knowledge users are actively searching for, which lets Use Case administrators identify their interests and potential knowledge gaps in the system.
    
    \item \textbf{Topics from retrieved documents:} This was given a \textbf{medium priority}. The feedback noted that because documents can be very large and cover many subjects, the detected topics might not be very precise, making this analysis less useful.
    
    \item \textbf{Topics from knowledge assets:} This was rated as a \textbf{low priority}. The team reasoned that Use Case administrators already know what their knowledge assets are about, so a visualization of these topics would likely offer little new insight.
\end{itemize}
\subsubsection{Task Type Analysis}
To analyze user intent and behavior, the concept of a "Task Type Map" was introduced. In contrast to the Topic Map, which focuses on \textit{what} users ask, this feature classifies queries based on \textit{how} users interact with the system. The primary goal is to understand the different ways the application is used, which can provide valuable insights for improving user workflows.

The initial concept proposed displaying this data in a bubble plot, similar to the Topic Map, where bubble size would represent the frequency of each task type. However, the team questioned this visualization during the workshop. They pointed out that the semantic relations of the different task types do not matter in this case, since only predefined task types will be assigned to the user queries, which makes a semantic interpretation of the different task types themselves obsolete. As a more practical alternative, the team suggested that a \textbf{bar chart} would be better suited to compare the frequencies of these distinct task categories.

However, making the most frequently used task types visible to the Use Case administrators can improve the \textit{Richness} and \textit{Information Quality} \parencite[pp.~57--58]{Jennex2006} by optimizing the RAG system and its underlying data to better fulfill these task types. It also enhances the \textit{User KM Service Quality} \parencite[pp.~58--59]{Jennex2006}. Use Case administrators can interpret how their users use the RAG system by analyzing the most relevant task types. This enables them to support their users better.

Despite the discussion on the best visualization method, the development team assigned the Task Type Analysis a \textbf{high priority} (see Tables \ref{tab:info_quality_p1}, \ref{tab:info_quality_g2_p1} in the appendix). They agreed that understanding the primary ways the system is used is important. This information can help to improve and streamline common user workflows and better tailor the system to user behaviors.
\subsubsection{Quality of Retrieval and Response}
An important concept for evaluating the system is the measurement of its output quality. This feature focuses on assessing both the relevance of the retrieved documents and the quality of the generated response. The main goal is to establish metrics for how well the system performs to ensure that users receive accurate and helpful answers. By analyzing the quality evaluations, Use Case administrators can draw conclusions about how well the system performs in terms of information \textit{Richness} and \textit{Information Quality} \parencite[pp.~57--58]{Jennex2006}, identify trends and also conclude how their own past decisions influenced the \textit{Information Quality}. This way, they can enhance it over time.

The discussion highlighted that there is no single and perfect way to measure quality automatically. Therefore, the team proposed different approaches that could suit the quality measurement:

\begin{itemize}
    \item \textbf{AI as a Judge:} Using a separate LLM or another AI model to automatically score the quality of a given response.
    \item \textbf{Binary Metrics:} Implementing simple, automated checks. For retrieval, this could mean checking if any retrieved text has a certain similarity score to the query. For the response, it could be a check to see if an actual answer to the question was provided or if the LLM responded with an answer that states that it does not know the answer to the question.
    \item \textbf{User Behavior Analysis:} Using indirect metrics, such as counting the number of follow-up or rephrased questions. A high number could suggest that the initial response was unsatisfactory.
\end{itemize}

The results would be displayed in a time-series chart with the x-axis representing the time and the y-axis representing the quality of the system's responses. The system factors (the model selection, Query Splitting and Reformulation) are also included as filtering options, as described before, to see how they affect quality.

The development team unanimously assigned this feature a \textbf{high priority} (see Tables \ref{tab:info_quality_p1}, \ref{tab:info_quality_p2}, \ref{tab:info_quality_g2_p1}, \ref{tab:info_quality_g2_p2} in the appendix), describing it as one of the most critical factors for the RAG success. The team emphasized that guaranteeing high-quality responses is essential for user retention - if users do not get good answers, they will stop using the system.
\subsubsection{Combined Quality and Topic/Task Type Analysis}
Another concept proposed in the workshop was a combination of the previous ideas on quality measurements and Topic/Task Type Analysis into a single visualization. This "Combined Quality and Topic/Task Type Analysis" was designed to connect the system's quality directly with the specific topics or task types being handled. The goal is to identify patterns and understand if the \textit{Richness} and \textit{Information Quality} \parencite[pp.~57--58]{Jennex2006} vary for different topics/task types in user queries. By directly connecting output quality to topics/task types, Use Case administrators can pinpoint \textit{Information Quality} issues and improve the underlying data, as well as the RAG system for the topic/task type, if needed.

This feature uses the bubble plot from the "Topic Map" feature as its foundation. In the idea discussed in the workshop, the positioning of the bubbles (semantic similarity) and their color (topic clusters) remain the same, as does the time filter for dynamic analysis. The crucial change lies in the meaning of the bubble size. Instead of representing the frequency of a topic, the size of each bubble now represents the average retrieval or response quality for that topic. For example, a large bubble could signify a topic with consistently high-quality answers, while a smaller bubble might indicate a topic where the system often performs poorly. In case of the Task Type Analysis, a new way of connecting the task types to the quality needed to be developed during the implementation, since the concept of a map to display task types was rejected

The development team assigned this combined view a \textbf{high priority} (see Tables \ref{tab:info_quality_g2_p1},  \ref{tab:info_quality_g2_p2} in the appendix). The key benefit they identified was the ability to find patterns between query types and the quality of the results. By using this map, administrators could quickly see if a specific topic area suffers from low-quality answers or if certain task types are handled better than others. This information is critical for making targeted improvements and focusing development efforts on the areas where the system is weakest.
\subsubsection{Combined Task Type and Topic Analysis}
Another concept was proposed to analyze the relationship between the topics users ask about and the tasks they are trying to perform. This feature is designed to identify the specific user intents that are associated with different topics. This provides a deeper understanding of user behavior within different knowledge domains, which allows Use Case administrators to improve the \textit{Richness} and \textit{Information Quality} \parencite[pp.~57--58]{Jennex2006} by optimizing the RAG-system and its underlying documents, as well as the \textit{User KM Service Quality} \parencite[pp.~58--59]{Jennex2006} by allowing them to understand better how users use the system.

For this analysis, a new visualization method was suggested: a heatmap. In this chart, the rows would represent the various topics identified in the system, while the columns would represent the different task types (e.g., knowledge query, summarization). The color intensity of the cell where a topic and task type intersect would indicate how frequently that specific combination has occurred. For example, a darker cell would show that a particular task is performed very often in connection with a given topic.

The team agreed that this analysis could provide "good insights" into user behavior and assigned a \textbf{medium to high priority} (see Tables \ref{tab:info_quality_g2_p1}, \ref{tab:info_quality_g2_p2} in the appendix). Understanding which tasks are most common for high-interest topics could help administrators better structure content or create more relevant "Quick Actions" to improve the user experience for those specific topics.
\subsection{Mostly Service Quality related Concepts}
\subsubsection{User Interactivity Tracking}
To evaluate the \textit{Service Quality} \parencite[p.~18]{DeloneMcLean2003ISSuccessTenYearUpdate} of the application, a concept for tracking user interactivity was discussed. The idea is to monitor and count a wide range of specific user events that happen during a session. This approach tries to make the whole user interaction interpretable and therefore includes analyzing interactions like copying an answer, viewing a source document, or providing direct feedback. This enables Use Case admins to understand how their users use the system and, by this, improves their ability to support their users, which means that the \textit{User KM Service Quality} \parencite[pp.~58--59]{Jennex2006} is improved. Furthermore, since the use of the RAG system is voluntary, we can associate the actual \textit{Use} of the system and its functionalities with the \textit{Intention to Use}, which is a strong indicator for \textit{Net Benefits} and IS Success overall \parencite[pp.~23--24]{DeloneMcLean2003ISSuccessTenYearUpdate}.

The data would be visualized in a time-series diagram, showing the frequency of these different events over a selected period. A key aspect of this concept is to also provide the Use Case admins with context or an interpretation of what each measurement for the different interaction types could mean. For example, a high number of users opening source documents might indicate they are actively verifying the system's answers, while frequent follow-up questions could suggest that the initial answers lacked sufficient detail.

The team assigned this feature an overall \textbf{medium priority} (see Tables \ref{tab:service_quality}, \ref{tab:service_quality_g2} in the appendix), which led to a detailed discussion about which specific events were the most valuable to track.

The events considered most important were:
\begin{itemize}
    \item \textbf{Opened Source Documents:} This was rated \textbf{high} because it clearly shows if users find the retrieved sources relevant enough to investigate further.
    \item \textbf{Provided Feedback (Positive/Negative):} This was also seen as \textbf{high priority} because it is the most direct way to capture user sentiment and serves as a "good measurement" of perceived quality.
    \item \textbf{Follow-up Questions:} This was considered a useful indicator of answer quality, as a high frequency might signal that the initial responses were incomplete.
\end{itemize}

A specific challenge that arose from tracking user interactivity was how to correctly interpret follow-up questions. The team needed a way to distinguish between "deep dive" questions, which indicate positive user engagement and questions asked because the system's previous response was unclear or unhelpful. Two main approaches were considered: using a simple heuristic (e.g., assuming system-suggested "follow-up questions" are deep dives, while user-rephrased "alternative questions" are not) or employing an AI model as a "judge" to classify the question's intent based on context.

The recommendation from the workshop was to adopt a pragmatic hybrid strategy (see "Recommendation for the distinction between deep-dive follow-up questions and follow-up questions due to an insufficient answer" section \ref{section:recommendation-follow-ups} in the appendix). The consensus was to start with the "simple distinction" method first, as it is easy to implement and has no operational cost. If this heuristic proves insufficient for reliably separating the two question types, the team could then explore using an AI judge as a second step. This approach was favored due to concerns that the AI judge would be expensive to run and might not guarantee reliable results.
\subsubsection{Knowledge Gap Detection}
The last concept proposed for measuring \textit{Service Quality} \parencite[p.~18]{DeloneMcLean2003ISSuccessTenYearUpdate} is "Knowledge Gap Detection," which aims to identify areas where the system or the users may lack adequate knowledge. The goal is to highlight two distinct types of knowledge gaps:

\begin{enumerate}
    \item \textbf{Knowledge gap within the system:} This occurs when the topic of a user's question aligns with the topic of a relevant knowledge asset, but the system fails at retrieving relevant documents. This suggests an internal deficiency in the system's ability to retrieve the correct information, despite the knowledge theoretically existing within its accessible sources. This could be due to issues with indexing, retrieval algorithms, or the structuring of the knowledge assets themselves. In this case, Use Case admins can improve the \textit{Richness} and \textit{Information Quality} \parencite[pp.~57--58]{Jennex2006} of the RAG system by improving the RAG pipeline itself or the underlying data.
    \item \textbf{User knowledge gap:} This refers to situations where the topic of the user's question does not align with the topic of any relevant knowledge asset. This indicates that the user is searching for information that is either not present in the system's knowledge base or is not categorized in a way that allows for effective retrieval. This gap highlights potential areas where user training on the system scope might be beneficial. In this case, Use Case admins can improve the \textit{User KM Service Quality} \parencite[pp.~58--59]{Jennex2006} of the RAG system by assisting their users in choosing a suitable Use Case for their requests.
\end{enumerate}

For visualization, it is proposed to track the occurrences of these knowledge gap types over time and display them in a time-series diagram. The x-axis would represent time, while the y-axis would indicate the frequency of each gap type. This allows administrators to observe trends in knowledge gaps and identify if certain periods or topics are more prone to these issues.

During the workshops, the development team assigned a \textbf{medium to high priority} to the "Knowledge gap detection" feature, specifically emphasizing its importance for analyzing system data (see Tables \ref{tab:service_quality}, \ref{tab:service_quality_g2} in the appendix). The ability to identify internal system knowledge gaps was considered crucial for pinpointing areas where the RAG mechanism or the underlying knowledge base needs improvement. The mechanism for checking topic alignment -- whether topics "fit together" -- was discussed. The proposal uses semantic similarity measures (e.g., cosine similarity of embeddings) to compare the topics extracted from user queries, retrieved documents and knowledge assets. If the similarity score falls below a predefined threshold, a mismatch and thus a potential gap, is flagged.
\subsection{Implementation Roadmap}
Finally, after discussion with the development team, the following roadmap for implementing the proposed software features was agreed upon:
\begin{enumerate}
    \item Implementation of the tracking system for user interactions
    \item Implementation of a (caching) logic for performance optimization
    \item Topic Maps
    \item Quality of retrieval/responses
    \item Combination of quality and topic/task type maps
    \item Task type maps
    \item Knowledge gap detection
    \item Combination of task types and topics
    \item Interactivity analysis
    \item Response time analysis
    \item Error rate analysis
\end{enumerate}
This roadmap was particularly important because it was clear from the beginning that not all of the proposed features could be implemented within the timeframe of this thesis due to time constraints. Therefore, prioritizing the concepts was essential to ensure that the most important features could be developed and integrated first.
\section{Description of the implemented Software Features}
The following section describes how the software features were implemented in the RAG system. For each feature, the description will cover its integration into the user flow as well as its technical implementation.
\subsection{Tracking System and GDPR Compliance}
The first thing we need to evaluate the user interactions is a tracking system to record them. The foundation of the tracking system developed during the thesis is BigQuery \parencite{GoogleBigQuery}, a scalable, high-performance data warehouse solution from Google. As it is natively integrated into the GCP, it is well-suited for analyzing the application's user data.

To record the data, a Google Cloud Run service \parencite{GoogleCloudRunService} is used, which receives data from the RAG application's Vue-based \parencite{vuejs} frontend via HTTP and writes it to BigQuery. These HTTP calls are executed asynchronously and in parallel with the rest of the application's requests. This ensures that the tracking process does not interfere with the performance or usability of the RAG application.
\\
The tracking system supports the recording of the following interaction types:
\begin{itemize}
    \item \textbf{Question Asked:} A user submits a new query.
    \item \textbf{Documents Received:} The system retrieves a set of source documents.
    \item \textbf{Answer Received:} The LLM generates a response.
    \item \textbf{Quick Action Used:} A user uses Quick Action.
    \item \textbf{Alternative Question Asked:} A user asks a system-suggested alternative question.
    \item \textbf{Follow-Up Question Asked:} A user selects a system-suggested follow-up question.
    \item \textbf{Source Document Opened:} A user opens a source document to view its contents.
    \item \textbf{Feedback Provided:} A user gives feedback on a generated answer.
    \item \textbf{Model Selected:} A user selects a language model for the conversation.
    \item \textbf{Query Reformulation Used:} The query reformulation feature is activated.
    \item \textbf{Query Splitting Used:} The query splitting feature is activated.
    \item \textbf{Copy Button Clicked:} A user copies the text of the generated answer.
    \item \textbf{Filtering Used:} A user applies a metadata filter for the search.
\end{itemize}
To maintain the context of a conversation, interactions belonging to a Chat or Thread within a Use Case are also tracked. In BigQuery, the Chat is recorded as a "SessionChat" and the Thread is recorded as a "Session". This structure makes it possible to determine which interactions were triggered within the same Chat or Thread.

For compliance with GDPR \parencite{GDPR_2016} regulations, these "SessionChats" and "Sessions" are pseudonymized. This means that pseudonymized IDs are used to prevent a "SessionChats" or "Session" from being linked back to a specific user, removing the possibility of subsequent re-identification. Furthermore, a table was created containing a justification for why the recording is necessary for every single data point that is tracked to make the tracking system GDPR compliant (see Table \ref{tab:gdpr_compliance_en} in the appendix).
\newpage
\subsection{Data Flow and Data Preprocessing}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/DataFlow_Bachelorthesis.drawio.png}
    \caption{Dataflow, for reference to the used icons see in the Image Credits section \ref{section:image_credits} in the appendix}
    \label{fig:enter-label}
\end{figure}

The data flow for the dashboard system, as illustrated in the preceding diagram, can be broken down into three primary processes:

\begin{enumerate}
    \item \textbf{Data Collection:} As described before, when a user interacts with the system, these interactions, such as asking a question or clicking a button, trigger an event that is sent via an HTTP request to a Cloud Run Service. The purpose of this service is to receive these events and insert the raw interaction data into BigQuery.

    \item \textbf{Data Preprocessing:} To ensure the dashboards are performant and can display analytics quickly, the data is preprocessed since some of the calculation steps have a long runtime. To do so, an automated preprocessing job is run. A Cloud Scheduler \parencite{googlecloudscheduler} is configured to trigger a Cloud Run Service at configurable regular intervals. This service reads the raw data from BigQuery, performs the necessary calculations to prepare the data for displaying in the dashboard and writes the processed data back into new tables within BigQuery.

    \item \textbf{Data Visualization:} When a user opens a dashboard in the Vue Frontend, a request is sent to a Cloud Run Service that functions as a backend for the dashboard. This service queries the preprocessed data from BigQuery. It then sends this data back to the Vue Frontend, which renders the final charts for the user using the Apache eCharts \parencite{apache_echarts} library. This architecture ensures that user requests are served quickly, as the complex data processing has already been handled before.
\end{enumerate}
\subsection{The Welcome Page}
The first page a Use Case administrator sees when accessing the dashboard is the Welcome Page (see Figure \ref{fig:welcome_page} in the appendix). This page serves as a central starting point, providing instructions on how to use the dashboard system and offering short descriptions for each of the available analytical metrics. As shown in the screenshot, the user receives guidance on the general workflow in a "Getting Started" section. Above this, cards introduce the different metrics of the dashboard. From here, the Use Case administrator can begin their analysis by selecting one of the metrics to explore.
\subsection{Answer Quality}
\subsubsection{User Flow}
The Answer Quality feature was implemented to provide Use Case administrators with a time-series chart, as outlined in the initial feature concept (see Figure \ref{fig:answer_quality} in the appendix). This chart allows them to observe trends in Answer Quality over time and draw conclusions about the success of their own measures. The diagram displays the four quality metrics: Relevance, Completeness, Accuracy and Faithfulness. A description for each metric can be found in the prompt template provided in the appendix (see Listing \ref{lst:prompt_template} in the appendix). Furthermore, an end-user study should be conducted to validate the choice and correct calculation of these metrics. To provide a clearer overview, the values are smoothed to display the daily averages for the selected period. In addition to filtering by Use Case and date range, the dashboard also displays the overall average values for each metric above the chart, enabling a quick analysis of the entire period. The chart is interactive, allowing users to hide individual lines for better focus and use tooltips to analyze specific data points in detail.
\subsubsection{Technical Implementation}
During the workshops, three technical options were proposed for this feature: evaluation using binary metrics, analysis of user behavior, or using an AI model as a "judge". The binary metric and user behavior analysis were considered to be less accurate proxies for quality, as both methods only indirectly evaluate the content of the generated response. This is obvious in the case of user behavior analysis, which only looks at the user's subsequent questions, not the answer itself. In this way, a detailed explanation of the answer quality is not possible. The binary analysis is also limited, as it primarily checks if the RAG system could assemble relevant information, but does not assess the qualitative aspects of that information.

Therefore, the decision was made to implement the AI evaluation variant. In this implementation, a language model is provided with the user's query, the retrieved source documents and the final answer during the preprocessing step of the data flow. Using a prompt template (see Listing \ref{lst:prompt_template} in the appendix), which contains detailed instructions for determining the different quality metrics, the AI "judge" then assesses the answer and calculates the scores. The calculated scores can then be accessed from the frontend via an HTTP request to the Cloud Run backend that accesses the results stored in BigQuery.
\subsection{Document Quality}
\subsubsection{User Flow}
The Document Quality feature was implemented in a manner similar to the Answer Quality feature, based on the concepts developed in the workshops (see Figure \ref{fig:document_quality} in the appendix). For each user query, this dashboard displays a time series that shows several metrics related to the retrieved documents: the average, highest and lowest relevance scores, as well as the total number of documents. The technical basis for relevance is described in the next section. These values are presented as smoothed daily averages in a time-series chart to make it easier for the Use Case administrator to analyze trends. The dashboard also includes options to filter by Use Case and time period and the overall averages for each metric are displayed in cards above the chart. The chart itself is interactive, allowing administrators to use tooltips for detailed analysis of specific data points and to toggle the visibility of individual time series.
\subsubsection{Technical Implementation}
To determine the document quality metrics, the implementation uses the relevance scores that are calculated by the system's reranker during the retrieval process. This approach was chosen to avoid the additional costs associated with using another AI model for a separate evaluation. These reranker scores, which are generated by Google's Ranking API \parencite{GoogleReranker}, already provide an assessment of how well the retrieved documents match the user's query. From these scores, the system calculates the average, highest and lowest relevance values for each query, as well as the total number of documents retrieved in the preprocessing step. It can then request these aggregated values.
\subsection{Topic Maps}
\subsubsection{User Flow}
The Topic Maps are presented to the user as a bubble plot (see Figures \ref{fig:topic_maps_overview},  \ref{fig:topic_maps_maps} in the appendix). In contrast to the proposal from the workshop, in the implemented visualization, each bubble represents an individual user question -- not a whole topic -- allowing for a more detailed overview. The position of each bubble in the chart is determined by its semantic meaning, with topically similar questions appearing closer together than dissimilar ones. Furthermore, the questions are thematically grouped using a clustering algorithm. Each cluster of questions is assigned a unique color on the chart and is displayed with a descriptive title in the legend. The dashboard allows administrators to filter the view by Use Case and date range, enabling them to analyze how the thematic composition of user queries changes over time. Tooltips are also available to show details for each individual question bubble.
\subsubsection{Technical Implementation}
The Topic Maps are generated during preprocessing by first embedding all user queries using the \texttt{gemini-embedding-001} model in its clustering mode \parencite{GoogleEmbedding001}. These embeddings are then clustered using the HDBSCAN algorithm \parencite{Malzer2021}. To achieve robust results across varying amounts of data, the input parameters for HDBSCAN, specifically \texttt{min\_cluster\_size} and \texttt{min\_samples}, are dynamically adjusted based on the total number of queries being analyzed. This adaptive approach ensures sensible clustering for both small and large datasets. The HDBSCAN algorithm is a good choice for this use case as it automatically determines the number of topics (clusters) and effectively isolates irrelevant queries as noise. This eliminates the need to pre-specify the number of clusters, a key limitation of methods like K-means, making it ideal for discovering topics in an exploratory way \parencite[p.~5]{Baligodugula2025}. To overcome the issue that HDBSCAN is not well-suited for high-dimensional data \parencite[p.~5]{Baligodugula2025}, UMAP \parencite{McInnes2020} is applied before the HDBSCAN algorithm to reduce the dimensions to 50 beforehand.

After clustering, a language model uses a prompt template to generate a descriptive title for each cluster based on a sample of its queries; this title is then displayed in the legend. Finally, to visualize the 3072-dimensional vectors produced by the embedding model, their dimensionality is reduced to two dimensions using UMAP again, allowing them to be plotted on the 2D chart. The UMAP algorithm is known to handle high-dimensional vectors well, which makes it a good choice in the case of high-dimensional embedding vectors compared to, for example, t-SNE, which scales worse than UMAP \parencite[pp.~51]{McInnes2020}.
The resulting clusters, cluster names and 2-dimensional coordinates stored in BigQuery can be requested by the frontend using the Cloud Run backend.
\subsection{Task Type Analysis}
\subsubsection{User flow}
The Task Type Analysis was implemented as a time-series diagram (see Figure \ref{fig:task_types} in the appendix). It was noted in the workshop that a bubble plot would not fit this feature because the semantic relationship between the discrete task types is not important. A bar chart was suggested as an alternative. However, a time-series chart was ultimately implemented because, in addition to showing numerical values, it also allows for the analysis of trends over time. This chart displays the days on the x-axis and, on the y-axis, the number of times an individual task type occurred in user queries on a given day.

The system recognizes and displays the following task types:
\begin{itemize}
    \item Question Answering
    \item Summarization
    \item Content Creation
    \item Code Generation
    \item Rewriting
    \item Translation
    \item Data Analysis
    \item Other
\end{itemize}
\textbf{Note:} These task types are a collection of types that are recognized as typical task types for GenAI applications and could potentially appear in requests to the examined RAG system. Yet, they should be evaluated in an end-user study.\\
\\
The dashboard also provides options to filter by Use Case and time period. The chart is interactive, supporting tooltips for detailed analysis and the ability to toggle the visibility of each time series. Furthermore, to provide the at-a-glance summary that a bar chart would have offered, the overall percentage distribution of each task type for the entire selected period is displayed above the chart.
\subsubsection{Technical implementation}
Technically, a language model assigns each user query to a specific task type during preprocessing. This is done using a prompt template (see Listing \ref{lst:prompt_template} in the appendix) that contains detailed instructions and descriptions for each category. The prompt template receives the user's question, the final answer and the retrieved documents as input parameters. To save costs, the quality evaluation (as described previously) and the task type assignment were combined into a single prompt. Therefore, for this classification task, the language model receives more inputs than just the user query. In the end, the assigned task types can be requested by the frontend using the Cloud Run backend that accesses the results stored in BigQuery.
\subsection{Combining Answer and Document Quality with Topic Maps}
\subsubsection{User Flow}
In the Topic Map view, administrators can display collected document and answer quality scores directly on the bubble plot (see Figure \ref{fig:topic_maps_quality} in the appendix). Since each user question is represented as a single bubble and varying bubble sizes would be difficult to distinguish across all questions, the bubble size as proposed in the workshop isn't used as a quality indicator. Instead, when a quality metric is chosen from a dropdown menu, the bubbles change color. Greener colors indicate better quality (closer to 1), while redder colors signify poorer quality (closer to 0), replacing cluster membership as the color's meaning. Tooltips show the relevant quality metrics for each question.

Additionally, a detailed view is available, which presents a table with an overview of either the document or answer quality metrics for all questions within a specific cluster (see Figure  \ref{fig:topic_maps_details} in the appendix). The administrator can select both the cluster and the set of metrics to be displayed using dropdown menus.

This feature allows for the identification of specific knowledge gaps for certain topics within a Use Case. By analyzing quality in the context of topics, a Use Case administrator can determine which subjects are being answered well and which are not. If a topic is performing unexpectedly poorly, the administrator can use this analysis to recognize the issue and initiate countermeasures, such as adding new source documents to the knowledge base.
\subsubsection{Technical Implementation}
Technically, this feature is enabled through simple \texttt{JOIN} operations in the BigQuery queries. This allows the quality metrics for each question to be queried and merged with the Topic Map data at the time of the request. Preprocessing is not needed.
\subsection{Combining Answer and Document Quality with Task Types}
\subsubsection{User Flow}
The combination of quality and task types is also presented to Use Case admins as a time-series diagram (see Figure \ref{fig:quality_per_task_type} in the appendix). In this view, the administrator can select a specific quality metric (from either document or answer quality) using dropdown menus. The chart then displays the performance of this chosen metric over time, with each line representing a different task type (e.g., "Question Answering," "Summarization"). This allows for a direct comparison of how quality differs across various user intents.

As in the other dashboards, the data is smoothed by displaying daily averages. Users can filter the data by Use Case and time period. Above the chart, cards display the overall average values for the relevant quality metrics across the entire selected period. The chart itself is interactive, offering tooltips for a detailed daily breakdown of quality per task type and the ability to show or hide the lines for specific task types to focus the analysis.
\subsubsection{Technical Implementation}
Technically, the data for this view is queried using simple \texttt{JOIN} operations on the necessary tables in BigQuery during the user's request.
\subsection{Combining Task Types with Topics}
\subsubsection{User Flow}
The last implemented feature is the combination of task type and topic (see Figure  \ref{fig:task_type_per_topic} in the appendix). This analysis is presented to the user as a heatmap. In this visualization, the horizontal axis represents the individual task types, while the vertical axis represents the respective semantic clusters. Each question that belongs to a specific task type and a specific cluster is counted in the cell where the two overlap. Cells with higher total values are colored more darkly than cells with lower values. It is also possible to filter the display for a specific time period and to restrict the view to a single Use Case.
\subsubsection{Technical Implementation}
Here as well, the data is queried through simple \texttt{JOIN} operations on the necessary tables in BigQuery during the user's request.
\subsection{Relative operational Cost Comparison}
To assess the financial impact of the implemented analytics features, a detailed, relative comparison of operational costs per request was conducted. The analysis focused specifically on the additional LLM and embedding costs incurred by the new analytics pipeline for each user query. This additional cost was then compared to the baseline LLM cost of the main RAG query-response chain. To create a focused comparison, several cost factors were intentionally excluded, most notably data retrieval costs from the Vertex AI Datastore that cannot be traced back to a single request, as well as negligible costs from BigQuery and Cloud Run. This means the resulting percentages represent a \textbf{maximum relative increase}, since including the relatively high datastore costs would lower the final figure. The costs for the title generation for the clusters have been ignored too, as they are negligible at scale (many queries form a cluster) and cannot be traced back to a single query.

Two primary scenarios were analyzed to understand the cost impact under different conditions:

\begin{itemize}
    \item \textbf{Scenario 1: Simple RAG Chain} The baseline was a simple query using one call to the most suitable Gemini model for Question Answering (\texttt{Gemini-2.5-Pro}). The additional analytics features require one call to a lower-cost model (\texttt{Gemini-2.0-Flash}) for quality and task type evaluation and one call to the embedding model (\texttt{Gemini-Embedding-001}) for Topic Map analysis. In this scenario, the relative cost increase was calculated to be \textbf{24\%} \parencite{GoogleGeminiPricing}.

    \item \textbf{Scenario 2: Complex RAG Chain} This scenario assumed a more advanced baseline chain that already included features like Query Reformulation and Splitting, which require several additional calls to smaller, faster models. When the same analytics features were added to this more expensive baseline, their relative cost increase was lower, calculated at approximately \textbf{18\%} \parencite{GoogleGeminiPricing}.
\end{itemize}
Detailed calculations can be found in the appendix (see "Cost Calculations" section \ref{app:cost_calculations} in the appendix).

From these scenarios, it was concluded that the analytics features add approximately \textbf{20\%} to the direct LLM-related costs, depending on the complexity of the underlying RAG chain. It is important to contextualize this increase: the cost is comparable to that of other advanced features like query reformulation and query splitting. Therefore, the introduction of the new analytics features is not expected to impact the total operational budget significantly.
\section{Final Feedback from the Development Team}
Finally, the development team was asked to provide feedback on the implemented software features in a dedicated workshop. During this session, the fundamental goal of the thesis, evaluating GenAI Success, was recapped first. Following this, each new analytics feature was presented in a live demonstration, with an explanation of how it contributes to the overall evaluation goal. To conclude the workshop, the developers provided structured feedback by filling out a detailed questionnaire, the complete results of which can be found in the appendix.

Overall, the feedback from the development team was highly positive and constructive (see Tables \ref{tab:feedback_answer_quality}, \ref{tab:feedback_doc_quality}, \ref{tab:feedback_topic_maps}, \ref{tab:feedback_task_type}, \ref{tab:feedback_task_by_topic}, \ref{tab:feedback_quality_by_task}, \ref{tab:general_feedback} in the appendix). The general design, interactivity and visual presentation of the dashboards were praised, with comments describing them as well-designed and intuitive for technical users (see Tables \ref{tab:feedback_answer_quality}, \ref{tab:general_feedback} in the appendix). The analysis of the detailed feedback revealed several key themes and areas for potential improvement:

\paragraph{Clarity and User Onboarding} A primary theme that emerged was the need for enhanced clarity, particularly for non-technical administrators (see Table \ref{tab:feedback_doc_quality} in the appendix). Recurring suggestions included adding more detailed legends to explain what metrics like "Faithfulness" mean (see Table \ref{tab:feedback_answer_quality} in the appendix) and providing specific examples to help users interpret the scores (see Tables \ref{tab:feedback_answer_quality}, \ref{tab:feedback_doc_quality}, \ref{tab:feedback_topic_maps} in the appendix). It was also noted that explaining the interdependencies between metrics -- for instance, how "Accuracy" and "Completeness" might influence "Faithfulness" (see Table \ref{tab:feedback_answer_quality} in the appendix) -- could provide a deeper understanding, perhaps visualized as a funnel (see Table \ref{tab:feedback_answer_quality} in the appendix). Furthermore, the idea of adding automated, LLM-generated summary statements to explain the main takeaways from a chart in plain language was proposed to facilitate access (see Tables \ref{tab:feedback_quality_by_task}, \ref{tab:general_feedback} in the appendix).
\paragraph{Actionability} A strong desire was expressed to connect high-level insights not just to deeper analysis, but to specific, operational actions. While the team suggested adding drill-down capabilities to trace an aggregated metric to its source (see Table \ref{tab:feedback_doc_quality} in the appendix), their feedback went further. There was a clear call to bridge the gap between identifying a problem and implementing a solution. Specific proposals included creating functionality to generate a new "Quick Action" directly from an identified, frequent user task (see Table  \ref{tab:feedback_task_type} in the appendix), as well as the ability to overlay key events, such as software releases or pipeline updates, onto the quality trend charts (see Table \ref{tab:feedback_answer_quality} in the appendix). This would enable a direct correlation between development efforts and their impact on performance.
\paragraph{Visualization Optimization} While the charts were considered visually appealing (see Table \ref{tab:general_feedback} in the appendix), the team provided feedback on optimizing the choice of visualization for the specific data. Specific suggestions included making clusters in the Topic Map more visually distinct by drawing lines around them (see Table \ref{tab:feedback_topic_maps} in the appendix), using different shapes in line charts for better accessibility (see Table \ref{tab:feedback_answer_quality} in the appendix) and implementing more advanced user interface controls, such as an interactive "sliding window" to select date ranges directly within a graph (see Table \ref{tab:feedback_answer_quality} in the appendix).
\paragraph{Trust and Explainability in AI-Generated Metrics} The developers also provided a differentiated perspective on AI-based evaluations. They noted that an LLM evaluating its output is inherently prone to error (see Table \ref{tab:feedback_answer_quality} in the appendix) and recommended adding disclaimers to manage user expectations (see Table \ref{tab:feedback_answer_quality} in the appendix). However, their main concern went beyond simple accuracy to the need for explainability. There was a desire not just to see a score, but to understand the reasoning behind it -- why the model considered a particular answer to be of low quality (see Table \ref{tab:feedback_doc_quality} in the appendix). This feedback highlights a need for systems that provide not just metrics, but also insight into the AI's decision-making process.\\
\\
In conclusion, from the development team's perspective, the implemented features provide significant value for assessing GenAI's success and represent a strong foundation for future work (see Table \ref{tab:general_feedback} in the appendix). The feedback suggests that the primary focus for subsequent iterations should be on improving the onboarding experience for non-technical users and, most importantly, enhancing the actionability of the dashboards -- not only by enabling more granular drill-downs, but by creating direct pathways from analytical insights to operational improvements.

% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  Discussion  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter{Discussion}
The primary goal of this work was to develop an artifact in the form of a dashboard that automatically evaluates the success of an enterprise GenAI application and presents its findings to the Use Case administrators in a clear and meaningful way so that they can draw conclusions for the optimization of the RAG system. The discussion will begin by interpreting the key findings in the context of the initial research question. Afterwards, the limitations of the developed artifact and the research methodology will be critically examined. Finally, the implications for both theory and practice will be outlined, followed by an outlook on future research opportunities.
\section{Interpretation of the Results}
The central result of this thesis is the successful development and implementation of an analytics dashboard integrated into an existing enterprise RAG system. This artifact was designed to provide Use Case administrators with the necessary tools to evaluate and improve the success of their GenAI applications, directly addressing the initial research question.

\paragraph{Connecting to the ISSM} The implemented features can be directly mapped to the dimensions of the DeLone and McLean ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}, which served as the theoretical foundation for this work.
\begin{itemize}
    \item \textbf{Information Quality} is addressed through the \textit{Answer Quality}, \textit{Document Quality}, \textit{Topic Map} and \textit{Task Type Analysis} features. These dashboards provide direct insights into the relevance, accuracy and completeness of the information provided by the system and allow Use Case administrators to identify knowledge gaps by analyzing the topics users are asking about. This directly operationalizes the \textit{Richness} and \textit{Linkages} components of the extended KMS Success model \parencite[pp.~57--58]{Jennex2006}.
    \item \textbf{System Quality} \parencite[p.~64]{DeloneMcLean1992ISSuccess}, while not the primary focus of the implementation due to prioritization, was conceptualized through features like \textit{Response Time} and \textit{Error Rate monitoring}.
    \item \textbf{Service Quality} is reflected in the analysis of \textit{Task Types}, \textit{Topic Maps} and the combined views. Furthermore, concepts like the \textit{error rate monitoring}, \textit{knowledge gap monitoring} and \textit{interactivity analysis} have been conceptualized. By understanding how users interact with the system (e.g., which tasks they perform for which topics), administrators can better tailor the service, for example, by creating more relevant Quick Actions or improving user training.
\end{itemize}
Using the evaluation features of these three categories, Use Case administrators can draw conclusions on their users' \textit{Intention to Use} and actual \textit{Use}  as well as their \textit{Satisfaction} with the RAG system and ultimately evaluate the \textit{Net Benefits} of it \parencite[p.~24]{DeloneMcLean2003ISSuccessTenYearUpdate}. By doing so, the implemented dashboard provides a solid foundation for evaluating IS Success in a RAG context.\\
\\
The final workshop feedback confirmed that the development team saw a clear link between the implemented features and the goal of evaluating IS Success. The features were perceived as providing a solid foundation for a comprehensive "Knowledge Analytics Engine."

\paragraph{Functionality and Value Added} The implemented artifact successfully translates abstract quality dimensions into specific, measurable metrics. The most valuable feature is the Topic Map combined with quality metrics. It moves beyond simple quality or performance checks and allows Use Case administrators to ask, "For which specific topic is our system failing?" This is a highly actionable insight that can directly lead to improvements, such as adding specific documents to the knowledge base. In contrast, features like the standalone quality dashboards provide scores that are harder to interpret without context. They signal that a problem exists, but do not immediately point to the cause. Tracing these aggregated metrics to their sources by adding drill-down functionalities will improve the actionability of these features.
\section{Critical Reflection and Limitations}
The limitations of this thesis can be categorized into methodological and technical aspects.

\paragraph{Methodological Limitations}
\begin{itemize}
    \item \textbf{Evaluation Scope:} The evaluation of the artifact was conducted exclusively with the internal development team. While their feedback is valuable for assessing technical feasibility and the conceptual alignment with developer goals, it does not represent the perspective of the end-users -- the Use Case administrators. An end-user study in a real-world scenario should be the next step to validate and improve the dashboard's practical usefulness and apply DSR best practices \parencite{Peffers2007}. However, this was not possible during the thesis due to time constraints.
    \item \textbf{Partial Implementation}: Also, due to the time constraints of the thesis, not all features from the initial workshops could be implemented. Features such as \textit{Response Time Analysis}, \textit{Error Rate Monitoring} and \textit{Knowledge Gap Detection} were conceptualized but not implemented. This means the implemented artifact provides a view that is heavily focused on \textit{Information Quality}, while \textit{System Quality} and \textit{Service Quality} are less comprehensively covered.
\end{itemize}

\paragraph{Technical and Conceptual Limitations}
\begin{itemize}
    \item \textbf{Reliability of AI-based Evaluation:} The core of the answer quality evaluation relies on an LLM acting as a "judge." As pointed out by the development team, this approach has inherent limitations. An LLM evaluating another LLM's output can be unreliable and may not align with human judgment. Therefore, a disclaimer should be added to manage users' expectations.
    \item \textbf{Complexity vs. Intuitiveness:} The final feedback highlighted an important tension. The dashboards are powerful but also complex. A non-technical user will need to invest time to understand the metrics and how to interpret them. The need for good help features and user onboarding is a direct consequence of this observation.
\end{itemize}
\section{Implications for Practice and Theory}
Despite the limitations, this thesis offers significant implications for both practical application and academic research in the field of IS.

\paragraph{Practical Implications} The primary contribution for practice is a ready-to-use analytics tool that empowers organizations to make data-driven decisions about their enterprise GenAI systems. Use Case administrators can now:
\begin{enumerate}
    \item \textbf{Identify and address knowledge gaps within a Use Case} by seeing which topics are frequently requested but receive low-quality answers.
    \item \textbf{Optimize user workflows} by understanding the most common task types used and how they are used (e.g., for which topics). They can adapt use-case-specific information like the glossary to their users' needs.
    \item \textbf{Analyze the impact of their decisions} by using time series diagrams to identify influential events and match them against the changes they made.
    \item \textbf{Guide users to better outcomes} by analyzing their behavior. Suppose a user asks about a topic in a Use Case that does not support the topic. In that case, the administrator can identify that using the Topic Map and recommend a more suitable one to ensure they receive a high-quality answer.
\end{enumerate}
The cost analysis further showed that the financial impact of such an analytics suite is manageable, adding approximately 20\% to the LLM-related costs, which is a justifiable expense for the insights gained.

\paragraph{Theoretical Implications} This work contributes to the academic discourse by demonstrating an operationalization of the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} in the new context of enterprise RAG systems. It shows how the abstract dimensions of the model can be translated into specific, measurable metrics (e.g., "Faithfulness" as a metric for \textit{Information Quality}). By employing the DSR methodology \parencite{Peffers2007}, a well-structured framework has been used to create new artifacts reliably. Furthermore, this work adopts a new perspective on evaluating RAG applications shaped by the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}, which has not been used in prior research. Rather than focusing only on RAG quality, a more holistic perspective is taken -- based on the three quality dimensions of the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} -- which in turn guides the development of a dashboard that allows Use Case administrators to perform comprehensive analyses.
\section{Outlook and Future Research}
This thesis provides a solid foundation and opens up several paths for future research and development.

\textbf{Long-Term User Study:} The most important next step is to conduct a long-term study with Use Case administrators using the dashboards in a real-world scenario. This would enable an evaluation of the artifact's real-world impact on IS Success \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} and provide valuable feedback for further refinement.\\
\\
\textbf{Refining AI-based Evaluation:} Future research could focus on improving the reliability and choice of the automated quality evaluation. This could be one part of the Long-Term User Study, where the Use Case admins could assess the usefulness of each quality metric.\\
\\
\textbf{Actionability:} A key suggestion from the feedback was to enhance actionability. Future work could focus on "closing the loop" by adding more drill-down capabilities to the dashboard, allowing users to trace issues to their sources. For example, a detailed view could be added to the quality charts that allows users to identify problems with the RAG quality.\\
\\
\textbf{Expanding the Scope:} Finally, the unimplemented features from the roadmap, such as \textit{Error Rate Monitoring} and \textit{Knowledge Gap Detection}, are next steps for expanding the dashboard's functionality to provide an even more holistic view of GenAI success.\\
\\
In conclusion, the developed artifact represents a significant step towards the systematic and automated evaluation of enterprise GenAI applications. The journey from raw data to actionable insight is complex, but this work has laid a solid and promising foundation for future exploration and improvement.

% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  -- - Conclusion  --  --  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\chapter{Conclusion}
The objective of this thesis was to design and implement a system for automatically measuring the success of an enterprise-level RAG application from a new and more holistic perspective on RAG success, utilizing the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}. The project began with a series of collaborative workshops with the development team of the investigated RAG application to conceptualize a suite of analytics features tailored to the specific needs of the RAG system. Following this, a significant portion of these features was technically implemented, resulting in an integrated analytics dashboard within the application's admin panel. The technical solution was built on the GCP, leveraging BigQuery for scalable data warehousing, Cloud Run for efficient data processing and a Vue-based frontend for interactive data visualization.

The implemented system provides insights into the three quality dimensions of the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate} - \textit{System Quality}, \textit{Information Quality} and \textit{Service Quality}. Key implemented features include dashboards for \textit{Answer Quality} and \textit{Document Quality}, which utilize AI-based evaluation to provide metrics such as relevance and Faithfulness. Furthermore, dashboards for \textit{Topic Map} and \textit{Task Type} analysis were developed to offer a deeper understanding of user interests and intents. These individual views were then combined into more powerful analytics tools that correlate answer or document quality with specific topics and task types, allowing administrators to pinpoint knowledge gaps and areas for improvement. A cost analysis revealed that these new features incur a manageable, maximum overhead of approximately \textbf{20\%} on LLM-related costs.

The implemented analytics dashboard was presented to the development team for evaluation. The feedback was very positive, with the team confirming the high value and potential of the features for monitoring and improving the RAG system. The design, interactivity and visual appeal of the dashboards were particularly praised.

The constructive feedback also highlighted several key areas for future enhancement. The most prominent theme was the need to improve clarity and user guidance for non-technical administrators through refined in-app explanations and user onboarding. A second major theme was the desire for greater actionability, primarily through the implementation of drill-down features that would allow users to trace an aggregated metric back to the specific documents or conversations that influenced it.

Due to the time constraints of this thesis, not all of the conceptualized features could be implemented. However, the roadmap and the developer's feedback provide a clear path for future development. Future work should prioritize the following.
\begin{itemize}
    \item Enhancing user guidance, for example, by implementing the suggested LLM-generated "insight statements" for each chart.
    \item Developing drill-down capabilities to make the data more actionable.
    \item Carry out an end-user study with the Use Case admins to test the dashboard in real-world scenarios
\end{itemize}

In conclusion, this work delivered a solid foundation for GenAI success measurement based on the ISSM \parencite{DeloneMcLean2003ISSuccessTenYearUpdate}. The implemented system provides the business with valuable insights into its RAG application and the detailed feedback from the development team creates a clear and validated path for future iterations to further increase its value.
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
% --  --  --  --  --  --  --  --  --  --  Appendix  --  --  --  --  --  --  --  --  -- --%
% --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- %
\appendix

\chapter*{Appendix}
\addcontentsline{toc}{chapter}{Appendix}           %Kapitel bekommt keine Nummerierung und wird trotzdem

% Fortsetzung der römischen Seitennummerierung
\pagenumbering{Roman}
\setcounter{page}{\value{seitenzahlroemisch}}

\section*{Statement on the use of AI for the creation of this thesis}
This paper was written independently and its language was revised with the assistance of Gemini and Grammarly. Cursor was used as a coding assistant during the artifact development.
\newpage
\section*{Workshop Feedback}
\subsection*{Group 1}
\begin{sideways}
    \begin{minipage}{\textheight}
        \centering
        \small 
        \captionof{table}{Workshop feedback on System Quality Metrics}
        \label{tab:system_quality}
        \begin{tabularx}{\linewidth}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
            \toprule
            \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
            \midrule
            
            \multirow{2}{=}{System Quality} 
            & 1 & Response Times & Medium & & & \\
            \cmidrule(l){2-7}
            & 2 & Error Rates & High & Errors will stop the user from using the wingman and there is a risk the user loses his interests in wingman. & & \\
            \bottomrule
        \end{tabularx}
    \end{minipage}
\end{sideways}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Information Quality Metrics (Part 1)}
    \label{tab:info_quality_p1}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule
        \multirow{6}{=}{Information Quality} 
        & 3 & Topic Map\dots & High & Different topics would result in different adjustments for better quality overall. & & need of large amount of data. Maybe not available in the current wingman stage \\
        \cmidrule(l){2-7}
        & 3.1 & \dots of queries & Medium & & & \\
        \cmidrule(l){2-7}
        & 3.2 & \dots of received docs & Medium & & & \\
        \cmidrule(l){2-7}
        & 3.3 & \dots of knowledge assets asked & High & & & \\
        \cmidrule(l){2-7}
        & 4 & Task type map & High & Task types can help to improve the workflow. & & \\
        \cmidrule(l){2-7}
        & 5 & Quality of retrieval/responses & High & it is very important to guarante good retrievals to keep the user using the wingman. & & there is no realiable way to identify the quality of retrieval/responses \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Information Quality Metrics (Part 2)}
    \label{tab:info_quality_p2}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule
        \multirow{6}{=}{Information Quality} 
        & 5.1 & LLM as a judge & Low & & & At the end it is a LLM correcting a LLM and therefore there may be a lack of quality in the rating. \\
        \cmidrule(l){2-7}
        & 5.2 & Binary & Medium & it is important to identify similar snippets. & & \\
        \cmidrule(l){2-7}
        & 5.3 & Amount of follow-up questions & Low & & & \\
        \cmidrule(l){2-7}
        & 6 & Combination of quality and topics/task types & High & important to identify a schema in the quality and the task types. & & \\
        \cmidrule(l){2-7}
        & 7 & Combination of task type and topic & Medium & & & \\
        \cmidrule(l){2-7}
        & 9 & Knowledge gap detection & Medium & & & \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Service Quality Metrics}
    \label{tab:service_quality}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule
        \multirow{12}{=}{Service Quality} 
        & 8 & Interactivity regardinng the amont of\dots & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.1 & \dots questions asked & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.2 & \dots copy-button pressed & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.3 & \dots quick action used & Low & & & \\
        \cmidrule(l){2-7}
        & 8.4 & \dots opened source & High & helps to identify if the retrieved source is interesting for the user. & & \\
        \cmidrule(l){2-7}
        & 8.5 & \dots started new thread & Low & & & \\
        \cmidrule(l){2-7}
        & 8.6 & \dots positive feedback provided & High & direct way for feedback very import. & & \\
        \cmidrule(l){2-7}
        & 8.7 & \dots negative feedback provided & High & & & \\
        \cmidrule(l){2-7}
        & 8.8 & \dots percentage of positive/negative feedback provided & High & good measurement. & & \\
        \cmidrule(l){2-7}
        & 8.9 & \dots changed model during a conversation & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.10 & \dots used websearch & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.11 & \dots deep-dive follow-up questions asked & Medium & can help to identify if the quality of the answers were good enough or not. & & \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}
\clearpage
\subsection*{Group 2}
\begin{sideways}
    \begin{minipage}{\textheight}
        \centering
        \small
        \captionof{table}{Workshop feedback on System Quality Metrics (Group 2 Feedback)}
        \label{tab:system_quality_g2}
        \begin{tabularx}{\linewidth}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X} % Hier \linewidth verwenden
            \toprule
            \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
            \midrule
            
            \multirow{2}{=}{System Quality} 
            & 1 & Response Times & Medium & Not that interesting but easy to measure; we assume they do not vary that much. & & \\
            \cmidrule(l){2-7}
            & 2 & Error Rates & Medium & Leads to faster error diagnosis. & Should probably only be a view for us as the dev team and not visible to customers. & \\
            \bottomrule
        \end{tabularx}
    \end{minipage}
\end{sideways}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Information Quality Metrics (Group 2 Feedback, Part 1)}
    \label{tab:info_quality_g2_p1}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule

        \multirow{6}{=}{Information Quality} 
        & 3 & Topic Map\dots & High & & & \\
        \cmidrule(l){2-7}
        & 3.1 & \dots of queries & High & Provides important insights into what knowledge is being requested. & & \\
        \cmidrule(l){2-7}
        & 3.2 & \dots of received docs & Medium & Less relevant because documents can be very large and as a result, topics cannot be captured very precisely. & & \\
        \cmidrule(l){2-7}
        & 3.3 & \dots of knowledge assets asked & Low & Provides relatively little insight, right? The assets should be reasonably clear, shouldn't they? & & \\
        \cmidrule(l){2-7}
        & 4 & Task type map & High & Interesting to see in what way the Wingman is primarily used. & Perhaps the presentation as bubbles? Task types are mainly knowledge queries and Quick Actions, where variance isn't very interesting. A bar chart would be a good alternative. & Categorization. \\
        \cmidrule(l){2-7}
        & 5 & Quality of retrieval/responses & High & Pretty much the most important thing. & & \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Information Quality Metrics (Group 2 Feedback, Part 2)}
    \label{tab:info_quality_g2_p2}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule

        \multirow{6}{=}{Information Quality} 
        & 5.1 & LLM as a judge & High & & & Costs? \\
        \cmidrule(l){2-7}
        & 5.2 & Binary & High & & & Hard to find a good threshold for relevance? \\
        \cmidrule(l){2-7}
        & 5.3 & Amount of follow-up questions & Low & It's just very hard to interpret. For example, follow-up questions can also happen simply by overlooking the alternative questions. & & \\
        \cmidrule(l){2-7}
        & 6 & Combination of quality and topics/task types & High & & & \\
        \cmidrule(l){2-7}
        & 7 & Combination of task type and topic & High & Also a good insight. & & \\
        \cmidrule(l){2-7}
        & 9 & Knowledge gap detection & High & Knowledge gap detection for system data is very important, the other less so. & & How is it checked if the topics fit together? \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}

\begin{sidewaystable}
    \centering
    \small 
    \caption{Workshop feedback on Service Quality Metrics (Group 2 Feedback)}
    \label{tab:service_quality_g2}
    \begin{tabularx}{\textheight}{p{2.2cm} l >{\RaggedRight\arraybackslash}X l >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X >{\RaggedRight\arraybackslash}X}
        \toprule
        \textbf{Category} & \textbf{\#} & \textbf{Name} & \textbf{Priority} & \textbf{What do you like?} & \textbf{What should be changed?} & \textbf{Which difficulties do you see regarding the implementation?} \\
        \midrule

        \multirow{8}{=}{Service Quality} 
        & 8 & Interactivity regardinng the amont of\dots & Medium & & & \\
        \cmidrule(l){2-7}
        & 8.1 & \dots questions asked & Low & Already in the Analytics Dashboard. & & \\
        \cmidrule(l){2-7}
        & 8.2 & \dots copy-button pressed & & & & \\
        \cmidrule(l){2-7}
        & 8.3 & \dots quick action used & & & & \\
        \cmidrule(l){2-7}
        & 8.4 & \dots opened source & & & & \\
        \cmidrule(l){2-7}
        & 8.5 & \dots started new thread & & & & \\
        \cmidrule(l){2-7}
        & 8.6 & \dots positive feedback provided & Low & Already in Feedback Dashboard. & & \\
        \cmidrule(l){2-7}
        & 8.7 & \dots negative feedback provided & Low & Already in Feedback Dashboard. & & \\
        \bottomrule
    \end{tabularx}
\end{sidewaystable}
\clearpage % This command forces all the tables above to be printed before continuing.
\subsection*{Recommendation for the distinction between deep-dive follow-up questions and follow-up questions due to an insufficient answer}
\label{section:recommendation-follow-ups}
I would use a hybrid strategy. First of all trying the simple distinction and if we cant seperate them than we could try AI as a judge. The problem with AI as a judge is that it is expensive and there is a risk that you dont get reliable results
\section*{Screenshots of the Dashboard}

%-- All figures go here --
\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/WelcomePage.png}
    \captionsetup{list=no}
    \caption{Welcome Page of the dashboard}
    \label{fig:welcome_page}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/AnswerQuality.png}
    \captionsetup{list=no}
    \caption{Answer Quality dashboard}
    \label{fig:answer_quality}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/DocumentQuality.png}
    \captionsetup{list=no}
    \caption{Document Quality dashboard}
    \label{fig:document_quality}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TaskTypes.png}
    \captionsetup{list=no}
    \caption{Task Types analysis dashboard}
    \label{fig:task_types}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TopicMapsOverview.png}
    \captionsetup{list=no}
    \caption{Topic Map dashboard overview}
    \label{fig:topic_maps_overview}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TopicMapsMap.png}
    \captionsetup{list=no}
    \caption{Topic Map in the Topic Map dashboard}
    \label{fig:topic_maps_maps}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TopicMapsDetails.png}
    \captionsetup{list=no}
    \caption{Detail view in the Topic Map dashboard}
    \label{fig:topic_maps_details}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TopicMapsQuality.png}
    \captionsetup{list=no}
    \caption{Topic Map dashboard with quality overlay}
    \label{fig:topic_maps_quality}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/QualityPerTaskType.png}
    \captionsetup{list=no}
    \caption{Quality per Task Type dashboard}
    \label{fig:quality_per_task_type}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.75\linewidth]{images/TaskTypePerTopic.png}
    \captionsetup{list=no}
    \caption{Task Type per Topic heatmap}
    \label{fig:task_type_per_topic}
\end{figure}
\clearpage
\section*{Prompt Templates}
\begin{lstlisting}[caption={Prompt Template for AI based quality and task type Evaluation}, label={lst:prompt_template}]
<General Instructions>
    You are an expert evaluator of human AI interactions.
Your task is to assess multiple aspects of the interaction between a human and an AI in a RAG system.
You should assess the task type of the user's question and the quality of the answer.
</General Instructions>

<Task Type Instructions>
Please evaluate the task type of the user's question. Only look at the question and not the answer or the supporting documents as these may distract you and are not relevant for the task type.
Then assign a task type to the question and determine a score between 0.0 and 1.0 that measures how well the answer fits to the task type. Generally try to assign a task type
The available task types are:
1. Youtubeing: Answer a question based on the provided text or document. This is the most common task type. Even if the system fails to answer the question, it is still a question answering task if a user asks a question that could potentially be found in documents.
2. summarization: Condense a long text into a shorter, coherent summary.
3. content_creation: Generate new content, such as articles, marketing copy, or stories.
4. code_generation: Create code in a specified programming language based on a natural language description.
5. rewriting: Rephrase or alter the provided text to change its style, tone, or complexity or other aspects.
6. translation: Translate text from a source language to a target language.
7. data_analysis: Interpret provided data to identify trends, generate insights, or create summaries.
8. other: Perform a task that does not fit into the other defined categories.
If you are not sure about the task type (task type score < 0.5), please choose the OTHER task type.
</Task Type Instructions>

<Answer Quality Rating Instructions>
Please evaluate the answer on the following four dimensions with a score between 0.0 and 1.0. Really think about the answer and the supporting documents and the question.
Here are the dimensions:
1. relevance_score: How well does the answer address the specific question asked? Does it stay on topic and provide information that directly relates to what was asked?
2. completeness_score: How comprehensive is the answer? Does it cover all aspects of the question or leave important parts unanswered?
3. accuracy_score: How factually correct is the information provided in the answer? Does it align with the supporting documents and contain accurate information? Does it fit to the common sense you know?
4. faithfulness_score: How well does the answer reflect the information from the supporting documents? Does it accurately represent the source material without adding unsupported claims?
</Answer Quality Rating Instructions>

<Conversation to assess>
Query: [QUESTION_PLACEHOLDER]

Answer: [ANSWER_PLACEHOLDER]

Supporting Documents:
[DOCUMENTS_PLACEHOLDER]
</Conversation to assess>

<Output Format>
IMPORTANT: Provide your evaluation as a JSON object with exactly the fields mentioned above in the following format:
{
    'task_type_response': {
        'task_type': 'Youtubeing',
        'task_type_score': x.xx
    },
    'quality_rating_response': {
        'relevance_score': x.xx,
        'completeness_score': x.xx,
        'accuracy_score': x.xx,
        'faithfulness_score': x.xx
    }
}
</Output Format>
\end{lstlisting}
\section*{GDPR Compliance}
\begin{longtable}{>{\RaggedRight}p{3.5cm} >{\RaggedRight}p{2.5cm} >{\RaggedRight}p{4.5cm} >{\RaggedRight}p{4.5cm}}
    \caption{GDPR Compliance Table}
    \label{tab:gdpr_compliance_en}\\
    
    \toprule
    \textbf{Data Field} & \textbf{Data Type} & \textbf{Description of the Data Field} & \textbf{Purpose of Collection} \\
    \midrule
    \endfirsthead
    
    \multicolumn{4}{c}{{\bfseries\tablename\ \thetable{} -- Continued}} \\
    \toprule
    \textbf{Data Field} & \textbf{Data Type} & \textbf{Description of the Data Field} & \textbf{Purpose of Collection} \\
    \midrule
    \endhead
    
    \midrule
    \multicolumn{4}{r}{{Continued on next page}} \\
    \midrule
    \endfoot
    
    \bottomrule
    \endlastfoot

    % -- Thread --
    \multicolumn{4}{l}{\textbf{Thread}} \\
    \multicolumn{4}{p{15cm}}{\textit{A Thread marks a cohesive usage session (User Journey).}} \\
    \cmidrule(r){1-4}
    Thread.id & UUID & Anonymous identifier for a cohesive User Journey. & Enables the anonymized analysis of a User Journey to improve System Quality by enhancing the data foundation and processing. \\
    Thread.created\_at & datetime & Timestamp of the thread's creation. & Serves the temporal analysis of user behavior, e.g., the frequency of new threads. \\
    Thread.use\_case\_id & str & If the thread uses knowledge assets to include external knowledge in the model's responses, it is assigned to a Use Case. This is the identifier for the Use Case. & Required for topic-specific evaluation to selectively improve the quality and data foundation for different Use Cases. \\
    \midrule

    % -- Chat --
    \multicolumn{4}{l}{\textbf{Chat}} \\
    \multicolumn{4}{p{15cm}}{\textit{A Chat is a single conversation within a thread and groups related interactions.}} \\
    \cmidrule(r){1-4}
    Chat.id & UUID & Anonymous identifier for a single conversation. & Serves anonymized analysis and quality assessment to optimize the underlying data template and data processing. \\
    Chat.type & ChatType & Classification of the conversation type, whether it uses external knowledge sources (ChatSearch) or not (Chat). & Necessary to differentiate system performance between general conversations and specific knowledge queries. \\
    Chat.thread\_id & UUID & Reference to the parent thread. & Technical necessity to analyze a conversation in the context of its entire user journey. \\
    \seqsplit{Chat.knowledge\_asset\_ids} & list[str] & List of identifiers for knowledge assets whose contents are used for the answers in this conversation. & Analysis of the usage and combined performance of different knowledge assets to maintain and qualitatively improve the data basis. \\
    Chat.created\_at & datetime & Timestamp of the chat's creation. & Enables the temporal classification of conversations for trend and progress analyses. \\
    \midrule

    % -- Interaction --
    \multicolumn{4}{l}{\textbf{Interaction}} \\
    \multicolumn{4}{p{15cm}}{\textit{An Interaction represents a single action by the user or the system, e.g., a question or an answer.}} \\
    \cmidrule(r){1-4}
    Interaction.id & UUID & Anonymous identifier for a single action. & Serves as a technical reference for reconstructing user flows and for detailed traceability of system decisions. \\
    Interaction.type & InteractionType & Type of user or system action (e.g., QuestionAsked). & Statistical basis for understanding interaction patterns. Necessary to optimize system logic and user guidance. \\
    Interaction.value & InteractionValue & Specific content of the respective interaction, the structure of which depends on Interaction.type. & Provides the content basis for statistical analysis to improve the data foundation and data processing models. \\
    Interaction.chat\_id & UUID & Reference to the parent conversation. & Technical necessity to assign an interaction to its specific conversation. \\
    \seqsplit{Interaction.previous\_interaction\_id} & UUID & Reference to the immediately preceding interaction. & Technical necessity to restore the chronological course of the conversation for analysis purposes. \\
    Interaction.created\_at & datetime & Timestamp of the interaction's creation. & Enables fine-grained, temporal analysis of individual user actions and system responses. \\
    \midrule

    % -- InteractionValue --
    \multicolumn{4}{l}{\textbf{InteractionValue}} \\
    \multicolumn{4}{p{15cm}}{\textit{An InteractionValue contains the specific content data of an interaction.}} \\
    \cmidrule(r){1-4}
    \seqsplit{QuestionAskedValue.question} & str & The query text entered by the user. & Analysis of questions and for quality optimization of the data basis, e.g., by identifying knowledge gaps. \\
    \seqsplit{DocsReceivedValue.docs} & list[Doc] & List of source documents that the RAG system considered relevant for the answer. & Serves to review and optimize the search and ranking algorithm. \\
    \seqsplit{AnswerReceivedValue.answer} & str & The answer text generated by the system and presented to the user. & Statistical analysis of the answers to evaluate and optimize the relevance and quality of the answer generation. \\
    \seqsplit{QuickActionUsedValue.quick\_action} & str & Identifier of the quick action used by the user. & Analysis of the frequency of use of quick actions to improve user-friendliness and functionality. \\
    \seqsplit{OpenedSourceValue.doc} & Doc & Details of the source document that the user explicitly opened for viewing. & Strong indicator of user-perceived relevance, crucial for validating and improving source ranking. \\
    GaveFeedbackValue & str, int... & Structured data of the explicitly submitted user feedback on an answer. & Processing of direct, voluntary feedback for targeted troubleshooting and prioritization of further developments. \\
    \seqsplit{SelectedModelValue.model} & str & Name of the language model selected by the user, if a choice is available. & Analysis of model preferences and the performance of different models in specific contexts. \\
    \seqsplit{UsedQueryReformulationValue.reformulated\_query} & str & Automatically reformulated search query for better searching within the database. & Analysis of the effectiveness of query optimizations. \\
    \seqsplit{UsedQuerySplittingValue.split\_subtasks} & list[str] & List of subtasks into which a complex query was divided for better processing. & Analysis of the usefulness and correctness of task decomposition to improve the understanding of complex user queries. \\
    \midrule

    % -- Doc --
    \multicolumn{4}{l}{\textbf{Doc}} \\
    \multicolumn{4}{p{15cm}}{\textit{A Doc (Document) represents a document used by the RAG system to answer queries.}} \\
    \cmidrule(r){1-4}
    Doc.title & str & Title of a source document. & Identification of documents for the analysis of source relevance and usage. \\
    Doc.url & str & URL that links to the source document. & Provision of a reference to the original source and analysis of the most frequently used information sources. \\
    \seqsplit{Doc.knowledge\_source\_id} & str & Identifier of the parent knowledge source from which the document originates. & Tracking of information origin to assess the reliability of different knowledge sources. \\
    Doc.quality\_score & float & System-calculated relevance score for the document regarding the query. & Serves the analysis and improvement of the RAG system's internal ranking algorithms. \\
    Doc.content & str & Relevant excerpt of the document used for answer generation. & Basis for answer generation and analysis of the relevance of specific text passages. \\
    \midrule

    % -- Enums --
    \multicolumn{4}{l}{\textbf{Enums}} \\
    \cmidrule(r){1-4}
    ChatType & Enum, str & Identifier for the type of a chat: "ChatSearch" for a chat including knowledge assets or "Chat" for a conversation without them. & Differentiation between different types of chat histories for analysis purposes. \\
    InteractionType & Enum, str & Identifier for the type of an interaction. Possible values: QuestionAsked, DocsReceived, AnswerReceived, QuickActionUsed, AlternativeQuestionAsked, FollowUpQuestionAsked, OpenedSource, GaveFeedback, SelectedModel, UsedFiltering, UsedQueryReformulation, UsedQuerySplitting, CopyButtonClicked, TranslationUsed. & Detailed differentiation of user interactions for fine-grained analysis of system behavior and user guidance. \\

\end{longtable}
\section*{Cost Calculations}
\label{app:cost_calculations}

This section illustrates the calculation of the approximately 20\% relative cost increase in LLM costs.
\subsection*{Methodology}
To be able to create a meaningful estimate, a simplification was applied. The following assumptions and limitations were made:
\begin{itemize}
    \item \textbf{Basis of Comparison:} The comparison is exclusively between the costs for LLM calls (inference) and embedding creation that arise.
    \item \textbf{Token Consideration:} Only the costs for input tokens are considered
    \item \textbf{Assumption of Equal Prompt Length:} It is assumed that the each prompt in the RAG chain and the evaluation prompt have approximately the same length. This is a simplification deemed sufficient for a high-level directional estimate.
    \item \textbf{Ignored Costs:} The following cost categories were intentionally not included in this calculation, as they are difficult to attribute to a single request or are considered negligible: \begin{itemize}
        \item Costs for Retrieval and Datastore in Vertex AI Search (difficult to attribute).
        \item Costs for BigQuery and Cloud Run (negligible).
        \item Costs for the generation of cluster names (negligible).
    \end{itemize}
    \item \textbf{Result as a Maximum Estimate:} Because retrieval costs in particular are ignored, the calculated values should be considered \textbf{maximum estimates} of the relative cost increase.
\end{itemize}
\subsection*{Pricing Basis}
The calculations are based on the prices for the respective Google Gemini models assumed in the project (per 1 million input tokens) \parencite{GoogleGeminiPricing}:
\begin{itemize}
    \item \textbf{Gemini-2.5-Pro:} \$1.25 / 1M tokens
    \item \textbf{Gemini-2.0-Flash:} \$0.15 / 1M tokens
    \item \textbf{Gemini-2.0-flash-lite:} \$0.075 / 1M tokens
    \item \textbf{Gemini-Embedding-0001:} \$0.15 / 1M tokens
\end{itemize}
\subsection*{Scenario 1: Basic Functionality}
In this scenario, advanced features like query reformulation and the generation of follow-up questions are deactivated. The RAG System only performs answer generation.

\paragraph{Costs of the RAG System (Baseline):}
\begin{itemize}
    \item 1 call to Gemini-2.5-Pro: \textbf{\$1.25}
\end{itemize}

\paragraph{Additional Costs from Dashboard Features:}
\begin{itemize}
    \item 1 call to Gemini-2.0-Flash: \$0.15
    \item 1 call to Gemini-Embedding-0001: \$0.15
    \item \textbf{Total Costs:} \$0.15 + \$0.15 = \textbf{\$0.30}
\end{itemize}

\paragraph{Relative Cost Increase:}
The percentage increase is derived from the ratio of the additional costs to the baseline costs.
\begin{equation*}
\frac{\text{Costs}_\text{new}}{\text{Costs}_\text{Baseline}} = \frac{\$0.30}{\$1.25} = 0.24 = \textbf{24\%}
\end{equation*}
\subsection*{Scenario 2: Full Functionality}
In this scenario, all features of the RAG System are activated.

\paragraph{Costs of the RAG System (Baseline):}
\begin{itemize}
    \item 1 call to Gemini-2.5-Pro: \$1.25
    \item 2 calls to Gemini-2.0-Flash: $2 \times \$0.15 = \$0.30$
    \item 2 calls to Gemini-2.0-flash-lite: $2 \times \$0.075 = \$0.15$
    \item \textbf{Total Baseline Costs:} \$1.25 + \$0.30 + \$0.15 = \textbf{\$1.70}
\end{itemize}

\paragraph{Additional Costs from Dashboard Features:}
\begin{itemize}
    \item 1 call to Gemini-2.0-Flash: \$0.15
    \item 1 call to Gemini-Embedding-0001: \$0.15
    \item \textbf{Total Costs:} \$0.15 + \$0.15 = \textbf{\$0.30}
\end{itemize}

\paragraph{Relative Cost Increase:}
\begin{equation*}
\frac{\text{Costs}_\text{new}}{\text{Costs}_\text{Baseline}} = \frac{\$0.30}{\$1.70} \approx 0.176 = \textbf{17.6\%}
\end{equation*}
\subsection*{Conclusion}
As the calculations demonstrate, the additional costs incurred by the evaluation features per request are in the range of \textbf{approximately 20\%}. The exact increase depends on which features are active in the underlying chain.
\section*{Evaluation}
\subsection*{Detailed Feature Feedback}

The following feedback was collected in a workshop with the development team. Participants rated the presented features based on three criteria on a scale of 1 (strongly disagree) to 5 (strongly agree):
\begin{itemize}
    \item \textbf{Is Intuitive to Use:} Rating of the feature's user-friendliness and clarity.
    \item \textbf{Fulfills Functionality:} Rating of whether the feature achieves its intended purpose.
    \item \textbf{Provides Added Value:} Rating of the feature's contribution to the overall goal of GenAI Success Evaluation.
\end{itemize}

The detailed scores and original comments are listed in the following tables.

% -- Table 1: Answer Quality --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Answer Quality}
    \label{tab:feedback_answer_quality}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Answer Quality (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5}: Es ist alles sehr klar und übersichtlich gestaltet. Ich finde es sehr gut designed. Ich finde mich gut zurecht. & \textbf{5}: \textit{Potential for Improvement:} Was ich cool fänd, ist wenn man die Zeitspanne per maus im Graphen auswählen kann. Wie so ein sliding window. Vielleicht könnte man auch für uns release dates/changes an der Pipeline in dem Chart markieren dann könnte man Qualität zu Änderungen an der Pipeline nachvollziehen. & \textbf{5}: Ich denke es ist sehr wichtig die Qualität der Antworten zu überwachen. \\
    \midrule
    2 & Product Owner & \textbf{3}: Mehr Legenden wären hilfreich, z. B. Score liegt zwischen 0 und 1, was genau bedeuten die Kategorien wie Treue. & \textbf{5}: Alle Werte sind gut ablesbar. \textit{Potential for Improvement:} Siehe oben, mehr Beschreibungen für Nutzerinfos. & \textbf{5}: Schafft mehr Transparenz für den Nutzer. \\
    \midrule
    3 & Customer Success Manager & \textbf{4}: Ich finde die Erklärung oben sehr gut! Die Skalen von 0-1 finde ich per se klar, aber die Bezeichnung der Skalen würde dennoch helfen. & \textbf{5}: \textit{Potential for Improvement:} Tranparenz hinsichtlich der Erklärung, wie sich die einzelnen Werte berechnen (non-technisch und nicht zu ausschweifend). & \textbf{5} \\
    \midrule
    4 & Manager & \textbf{4}: Ich überlege, ob es einen Zusammenhang zwischen den 4 kategorien gibt. Ist nicht eine hohe Treue, von hoher Genauigkeit und vollständigkeit abhängig? Falls ja, wäre das gut, dem Nutzer zu erklären, oder es vllt sogar wie einen Informations funnel drop-off darzustellen. & \textbf{4}: \textit{Potential for Improvement:} Ich denke nochmal auf die abhängigkeit der kategorien eingehen. & \textbf{5} \\
    \midrule
    5 & Entwickler/TPO & \textbf{3}: Man muss sich zumindest mal die Beschreibung gut durchlesen, die 4 Metriken sind jetzt glaube ich gerade für Laien (irgendwelche Gabelstaplerverkäufer zB) bestimmt nicht bekannt. & \textbf{4}: Gibt einen Überblick, aber da es durch LLMs erstellt wird, kann es glaube ich relativ random sein. Bzw ist die Frage so, was an Kontext alles dazu kommt. Also meine Vermutung wäre zB, dass ein menschlicher Experte wahrscheinlich oft anders scoren würde. \textit{Potential for Improvement:} Vielleicht noch die Möglichkeit die 4 Kategorien zu gewichten. Vielleicht ist mir ja was bestimmtes wichtiger. Und dann daraus einen "Superscore" basteln, der alle 4 Kategorien gewichtet enthält. & \textbf{2}: Glaube es ist besser als nichts, aber würde jetzt nur rein aus Intuition so vermuten, dass es wahrscheinlich nicht super akkurat ist. \\
    \midrule
    6 & Developer & \textbf{5}: Nutzung eindeutig. & \textbf{3}: LLM bewertet sich im Grunde selbst indem es seine eigene Antwortqualität benotet=> Ist zwar einfach + vollautomatisch umsetzbar aber dafür sehr fehleranfällig. \textit{Potential for Improvement:} Komplexere Evaluationspipeline bauen bspw. mit Inkorporation von Websuche + Self-reflection - sorgt dann halt auch wieder für deutlich höhere Kosten hahahah. & \textbf{4}: Automatische Qualitätsevaluation ist immer sinnvoll. \\
    \midrule
    7 & Software Entwickler & \textbf{3}: Ist villeicht nicht intuitiv genug für einen Nutzer. & \textbf{5}: Immerhin ist die funktion nützlich und es erfüllt die Funktionalität. \textit{Potential for Improvement:} Villeicht es intuitiver ohne graphen, sonder nur mit einem einzelnen score. & \textbf{5}: Die Funktion ist nützlich, es erstellt eine neue nützliche information der Antwortqualität zum verbessern des Produkts. \\
    \midrule
    8 & Softwareentwickler & \textbf{5}: - bekannte Diagrammart - Farben stimmen überein in der Übersicht mit den Durchschnittswerten und dem Diagramm - die Filter sind intuitiv verständlich. & \textbf{4}: Die gewünschte Funktionalität ist es, abzuschätzen zu können, ob (A) bereits erfolgte Maßnahmen zu einer gewünschten Qualitätssteigerung geführt haben und ob (B) andererseits bewerten zu können, ob weitere Maßnahmen notwendig sind. (A) kann gut erkannt werden ob der Trend nach oben geht (B) könnte schwierig werden, da der Nutzen ggf. nicht weiß, ab welchem Wert keine weitere Maßnahme notwendig ist -> welcher Wert ist "gut genug"? \textit{Potential for Improvement:} verschiedene Arten von Punkten (Kreis mit weißer Füllung, Quadrat, Rhombus, etc. pp.), um nicht nur anhand der Farbe die Linien unterscheiden zu können für (B) der Begründung, könnte man - falls es so etwas gibt - eine Art Threshold (vline) anzeigen (mit entsprechendem Tooltip, der das erklärt). & \textbf{5} \\
    \midrule
    9 & Softwareentwickler & \textbf{3}: Beispiele in einem Help Tooltip wären sinnvoll um die einzelnen Metriken nochmal genauer zu erläutern. & \textbf{5}: \textit{Potential for Improvement:} Prüfen wie gut die Evaluierungen durch das LLM sind und ob sie deswegen wirklichen Mehrwert bieten. Vielleicht noch einen Disclaimer hinpacken, dass diese Werte auch alle KI-generiert sind. & \textbf{4} \\
    \midrule
    10 & Developer & \textbf{5} & \textbf{5}: \textit{Potential for Improvement:} Titel des Diagramms finde ich zu generisch. Dann kann man ihn auch weglassen. Erklärung was Maßnahmen sein könnten, um Werte zu verbessern? Information wie genau die Werte ermittelt wurden? Ist 1 überhaupt realistisch erreichbar? Was wäre ein "guter" Wert, wäre vlt noch eine sinnvolle Information für den Nutzer. & \textbf{5} \\
\end{longtable}

\clearpage

% -- Table 2: Document Quality --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Document Quality}
    \label{tab:feedback_doc_quality}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Document Quality (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5} & \textbf{5}: \textit{Potential for Improvement:} Es wäre cool, wenn man noch fein granularer z.B. in einer Tabelle pro Use-Case die Metriken auf einer Dokumenten Ebene sehen könnte. & \textbf{5}: Absolut, ala garbage in garbage out. \\
    \midrule
    2 & Product Owner & \textbf{3}: Siehe vorhergehender Reiter (mehr Legenden, Beschreibungen). & \textbf{5} & \textbf{5} \\
    \midrule
    3 & Customer Success Manager & \textbf{3}: Skalen bezeichnen, y-Achse insbesondere, Vergleichbarkeit schaffen. & \textbf{3}: \textit{Potential for Improvement:} Beispiele zur Bewertung angeben, damit das Feature für non-technicals verständlich wird. & \textbf{2}: Dokumentenqulität sehr wichtig, die generelle Zahl hilft jedoch im Zweifel weniger als die Analyse einzelner Themenbereiche oder gar Dokumente. Vll kann man zwischen Sources unterscheiden? Dann wäre pro Use Case ein deutlicher Mehrwert gewonnen, weil ich eine genauere Einschätzung mit Bezug zu den tatsächlichen Dokumenten erhalte. \\
    \midrule
    4 & Manager & \textbf{4}: Die werte befinden sich teil über 1. anders als in der beschreibung. & \textbf{4}: \textit{Potential for Improvement:} ist es besser eine höhere anzahl verwendeter Dokumente zu haben, oder eine niedrigere? & \textbf{5} \\
    \midrule
    5 & Entwickler/TPO & \textbf{5}: Glaube "Relevanz" und "Anzahl gefundener Dokumente" sind eher Begriffe, die man auf Anhieb versteht. & \textbf{4}: \textit{Potential for Improvement:} Eventuell noch die Dokumente aus dem Retriever auch mit reinnehmen? (optional). Manchmal ist ja doch auch der Reranker einfach dumm, bzw gibt er auch oft ganz schön andere Scores als der Retriever. & \textbf{5}: Ist einfach zu verstehen und man kann denke ich es gut nutzen, um die Datenlage zu analysieren und daraus Schritte abzuleiten. \\
    \midrule
    6 & Developer & \textbf{5}: Nutzung eindeutig. & \textbf{3}: Hier sind die Scores am Ende halt auch wieder das Ergebnis von je einem LLM Call. Sowas wie Recall + Precision macht hier aber auch keinen Sinn da man dafür ja annotierte Docs bräuchte + Fragenset ... ist schwierig hahaha aber kann gut sein dass das mit vollautomatischer LLM-Evaluation das beste ist was geht. Das Problem ist halt hier dass das LLM (d.h. der Generator) am Ende des Tages den Retriever bewertet und es zumindest uns als Developern/Nutzern nur mitteilst dass die Dokumente die gefunden wurde gut/schlecht sind aber keinen tieferen Insight liefert WARUM jetzt gute/schlechte Dokumente gefunden wurden. & \textbf{4} \\
    \midrule
    7 & Software Entwickler & \textbf{3}: Wie schon vorher begründet, villeicht sind die Graphen zu komplex für einen Endnutzer, zu viele metriken können auch verwirrend sein. & \textbf{5}: Mir scheint es die gewünschte funktionalität zu erfüllen. \textit{Potential for Improvement:} Dumb it down for the end user. & \textbf{5}: Sieht gut aus, ist eine nützliche information, es kann benutzt werden für die verbesserung und entwicklung von wingman. \\
    \midrule
    8 & Softwareentwickler & \textbf{4}: selbe Punkte wie eben, ABER die Auswahl mit dem Diagrammmodus könnte verwirren bzw. übersehen werden, ggf. in die Einstellungsleiste oben (bei Datumsfiltern) mit aufnehmen. & \textbf{5}: Es sollte leicht ersichtlich werden, welche Dokumente am hilfreichsten sind. Aber meine Frage: Was bringt hier der Diagramm-Modus "Anzahl Dokumente"? Geht es nicht eigentlich nur um die Qualität? \textit{Potential for Improvement:} wie vorhin. & \textbf{5}: Ziel ist es, die relevantesten Dokumente hochzuladen, und das wird hiermit besser zu erkennen. \\
    \midrule
    9 & Softwareentwickler & \textbf{5}: Skala 0-1 leicht verständlich. & \textbf{5}: Pro Use Case kann man gut einsehen, ob relevante Dokumente vorhanden sind. \textit{Potential for Improvement:} Die Farben der Metriken finde ich nicht super intuitiv. Höchste Relevanz vielleicht lieber grün und niedrigste rot. & \textbf{5}: Dokumentenbasis kann basierend darauf erweitert werden. \\
    \midrule
    10 & Developer & \textbf{4} & \textbf{4}: \textit{Potential for Improvement:} Weiß ich, welches Dokument nicht gut funktioniert, wenn es mehrere pro Use Case gibt? Durchschnittsymbol bei höchsten und niedrigste Relevanz? Diagrammtitel zu generisch. Übersicht welche Dokumente dann wirklich auch top oder flop sind? Um wirklich Dokumente zu ändern, müsste man wissen, welche Dokumente ein Flop sind. Ein Diagramm über die Entwicklung der Anzahl der Dokumente sagt nicht viel aus oder? Ist das hier wirklich von Relevanz? & \textbf{3} \\
\end{longtable}

\clearpage

% -- Table 3: Topic Maps --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Topic Maps}
    \label{tab:feedback_topic_maps}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Topic Maps (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5} & \textbf{5} & \textbf{5}: Zu sehen, wo die Pipeline noch fehlschlägt und genau diese Edge Cases zu adressieren ist sehr sehr wichtig. \\
    \midrule
    2 & Product Owner & \textbf{3}: Siehe vorhergehender Reiter (mehr Legenden, Beschreibungen). & \textbf{5}: \textit{Potential for Improvement:} Cluster noch stärker hervorheben, dass Nutzer schneller verstehen, wie die Cluster zusammengesetzt sind und entstehen (z. B. noch kurze Beschreibung, nicht nur Titel?). & \textbf{5}: Hilft auch zu sehen, wie viele Anfragen wirklich thematisch gestellt werden und ob Nutzer gezielt zum Thema des Assets fragen. \\
    \midrule
    3 & Customer Success Manager & \textbf{3}: Gut, die Darstellung der Darstellung empfinde ein wenig als verwirrend, wegen fehlenden Skalen. & \textbf{5}: \textit{Potential for Improvement:} Prozent-Zahlen bzgl. Fragen zum Use Case / Clusterings / Knowledge Sourcen angeben \& ob diese im positiven / negativen Bereich liegen. Dies würde einen weiteren Mehrwert für den Kunden liefern. & \textbf{4} \\
    \midrule
    4 & Manager & \textbf{3}: Clustergröße sollte denke ich eher einen weniger technischen namen bekommen. Das Diagram ist etwas komplex, die Blöcke sind sehr einfach. & \textbf{4}: \textit{Potential for Improvement:} das diagram etwas vereinfachen. & \textbf{5} \\
    \midrule
    5 & Entwickler/TPO & \textbf{4}: Zumindest mit kleiner Intro glaube ich kann man das schon ganz gut verstehen, wenn man so bisschen Plan von Clustern hat. & \textbf{4}: Es gibt einen guten Überblick über Topics, die oft gefragt werden und zeigt auch direkt so, wie gut diese abschneiden. \textit{Potential for Improvement:} Cluster besser kennzeichnen (Kreis, Farbiger Hintergrund). Die Qualität lieber durch Größe der Punkte darstellen als durch verschiedene Grüntöne (war zumindest in der Präsi nicht wirklich zu erkennen). & \textbf{4}: Es gibt einen guten Überblick über Topics, die oft gefragt werden und zeigt auch direkt so, wie gut diese abschneiden. \\
    \midrule
    6 & Developer & \textbf{5}: coolstes Feature 10/10. & \textbf{5}: feier ich. & \textbf{5}: feier ich. \\
    \midrule
    7 & Software Entwickler & \textbf{3}: Wie von Casper vorgeschlagen, mit dem clustering können grenzen um die cluster gemalt werden um die cluster voneinander abzugrenzen. & \textbf{4}: Ich bin immernoch verwirrt dass fragen außerhalb des clusters bei denen wingman keine antwort gibt, gut evaluiert wurden. \textit{Potential for Improvement:} Man könnte gut evaluierte datenpunkte die vom Use Case cluster weit entfernt sind ignorieren, da sie falsch evauliert wurden. Man könnte auch die antwort beim clustering algorithmus und bei der evaluation benutzen, villeicht kann das auch das Problem lösen. & \textbf{5}: Guter Feature, macht sinn es zu haben. \\
    \midrule
    8 & Softwareentwickler & \textbf{3}: > dass Use Case Admins die Themen der Nutzeranfragen einschätzen können sehr gut erkennbar aber generell könnte es sein, dass dem Nutzer nicht klar ist, wie das Clustering zu lesen ist, also dass die Punkte einzelne Fragen sind und die Farben die unterschiedlichen Cluster; hier wäre vielleicht einfach ein Balkendiagramm, was für jeden (automatischen) Cluster anzeigt, wie häufig Fragen aus diesem Themengebiet kommen, wiederum die Cluster-Details unten sind sehr hilfreich und die Hauptstatistiken oben (Anzahl Fragen, durchschnittliche Cluster-Größe) sind auch sehr hilfreich. & \textbf{3}: > Use Case Admins die Themen der Nutzeranfragen einschätzen können SEHR GUT erkennbar > zudem auch die Antwort-/Dokumentqualität in den einzelnen Themenbereichen betrachten können Wo sieht man das? > Zudem kann abgeschätzt werden, ob die Nutzer überhaupt passende Fragen an einen Use Case stellen oder, ob ggf. Schulungen oder andere Dokumentenbasen notwendig sind. SEHR GUT erkennbar. \textit{Potential for Improvement:} - Cluster-Qualität gleich sortieren wie die Cluster in der Legende im Diagramm - ggf. das angesprochene Balkendiagramm. & \textbf{4}: sehr hilfreich um zu sehen wie die Nutzer den Use Case verwenden, allerdings frage ich mich noch, was die genaue Clusteransicht bringt, denn eigentlich ist doch eher die Anzahl/Verteilung interessant; und ich bin mir unsicher, wie hilfreich es ist, die Qualitätsbewertungen im Cluster anzuzeigen. \\
    \midrule
    9 & Softwareentwickler & \textbf{3}: Ein Cluster muss sich der User wahrscheinlich ein bisschen mehr gewöhnen. Toggle zwischen Clustern und Qualität ist noch nicht ganz optimal weil Cluster noch nicht umkreist / hinterlegt. Vielleicht könnte man auch die Clusterpunkte grün oder rot umranden damit man nur eine Ansicht hat. Die Qualität kann man dann ja ein und ausblenden lassen. & \textbf{4}: Aus den Tooltips ist teilweise Text noch über die Ränder gelaufen. Die Schwellwerte für die Farben der Zahlen unten in der Tabelle waren nicht symmetrisch nach unten und oben hin (ab 20\% rot und ab 70\% grün oder so). & \textbf{4} \\
    \midrule
    10 & Developer & \textbf{3}: Cluster schwer für Nutzer zu verstehen. Ein Beispiel mit Interpretation einbauen zum besseren Verständins? Wie bilden sich Cluster? So Fragestellungen sollten erklärt werden. Hinweis auf unterschiedlichen Anzeigemodus und wie die Grafik dann anders interpretiert werden sollte. & \textbf{4}: \textit{Potential for Improvement:} Cluster bildlich klarer darstellen, mit Umrandung z.B. & \textbf{5} \\
\end{longtable}

\clearpage

% -- Table 4: Task Type Analysis --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Task Type Analysis}
    \label{tab:feedback_task_type}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Task Type Analysis (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5} & \textbf{5}: \textit{Potential for Improvement:} Vielleicht eine Gügenüberstellung mit existierenden Quick Actions und Bonus Feature wäre, wenn man direkt aus einem häufig ausgeführtem Aufgabentyp eine QuickAction erstellen könnte. Direkt über einen link aus dem Interface. & \textbf{5} \\
    \midrule
    2 & Product Owner & \textbf{3}: Siehe vorhergehender Reiter (mehr Legenden, Beschreibungen). & \textbf{5} & \textbf{5} \\
    \midrule
    3 & Customer Success Manager & \textbf{3}: Skala wieder etwas unklar, ggf. Beispiel wie erhält man einen hohen Wert, wie einen geringen. & \textbf{4}: \textit{Potential for Improvement:} Für mich wieder Bezug zum Clustering/Source herstellen, weil die Auswertung dann auf einen greifbaren Themenbereich herstellen kann. & \textbf{4} \\
    \midrule
    4 & Manager & \textbf{4}: Farben sind hier nicht notwendig. Q&A ist grün (gut?) andere rot (schlecht?). & \textbf{4}: \textit{Potential for Improvement:} finde es ein sehr gutes feature, wie gut lässt es sich erweitern, das es nicht zu grobe kategorien gibt? & \textbf{5} \\
    \midrule
    5 & Entwickler/TPO & \textbf{5}: Super intuitiv. & \textbf{5}: Top. \textit{Potential for Improvement:} Glaube die Darstellung würde ich vielleicht ein bisschen anpassen. Finde der Zeitverlauf ist weniger Spannend als einfach obendrüber die Verteilung. Würde also glaube ich zentral eher so ein Balkendiagramm platzieren und das mit der Timeline mehr als sekundäres Diagramm nutzen. & \textbf{5}: Super nice Übersicht. \\
    \midrule
    6 & Developer & \textbf{5}: finde ich sinnvoll und gut gelöst. & \textbf{5}: finde ich sinnvoll und gut gelöst. & \textbf{3}: Nicht so relevant wie die ersten drei Analyse-Seiten. \\
    \midrule
    7 & Software Entwickler & \textbf{3}: Wie vorher, bin nicht von den graphen überzeugt. & \textbf{5}: Soweit die ausgewählte anzahl an Klassen alle möglichkeiten abdecken, passt es. \textit{Potential for Improvement:} Villeicht Dynamisch die Kalssen erzeugen (mit clustering algorithmus), vor allem bei der klasse "andere". & \textbf{5}: Nützliche information. \\
    \midrule
    8 & Softwareentwickler & \textbf{5} & \textbf{5}: \textit{Potential for Improvement:} in der Übersicht mit "Fragen insgesamt" etc. statt einem Layout mit nur einer Zeile und Scrollbar ein Layout was sich automatisch in mehrere Zeilen aufteilt, damit man immer alle Kategorien sieht, denn diese Übersicht ist immer sehr hilfreich für den ersten Überblich. & \textbf{5} \\
    \midrule
    9 & Softwareentwickler & \textbf{5} & \textbf{5} & \textbf{3}: Aufgabentyp "Sonstiges" sind sehr viele (vielleicht liegt es ja auch nur an den Testdaten). Allgemein weniger spannender Insight als die anderen. \\
    \midrule
    10 & Developer & \textbf{5} & \textbf{4}: \textit{Potential for Improvement:} Was umfasst Andere? Mehr Erklärungen für den Nutzer. & \textbf{4} \\
\end{longtable}

\clearpage

% -- Table 5: Task Type by Topic --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Task Type by Topic}
    \label{tab:feedback_task_by_topic}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Task Type by Topic (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5} & \textbf{5} & \textbf{5} \\
    \midrule
    2 & Product Owner & \textbf{3}: Siehe vorhergehender Reiter (mehr Legenden, Beschreibungen), an sich ist diese spezifische Übersicht aber eindeutiger als die vorherigen imo. & \textbf{5} & \textbf{5} \\
    \midrule
    3 & Customer Success Manager & \textbf{5}: Top! & \textbf{5}: Top! & \textbf{5}: Top! \\
    \midrule
    4 & Manager & \textbf{4}: Finde ich tatsächlich besser als nur "aufgabentyp" im verlauf. die grafik brint mir gefühlt mehr. & \textbf{4} & \textbf{4} \\
    \midrule
    5 & Entwickler/TPO & \textbf{5}: Super easy. & \textbf{5}: \textit{Potential for Improvement:} Eventuell muss man sich bisschen was überlegen, sobald es super viele Kategorien und Aufgabentypen sind? Also vielleicht noch mehr die wichtigsten Spots highlighten und vielleicht auch außerhalb der Heatmap gesondert in den Fokus rücken. (zB so "Top 3" oder so). & \textbf{5} \\
    \midrule
    6 & Developer & \textbf{5}: finde ich sinnvoll und gut gelöst. & \textbf{5}: finde ich sinnvoll und gut gelöst. & \textbf{3}: Nicht so relevant wie die ersten drei Analyse-Seiten. \\
    \midrule
    7 & Software Entwickler & \textbf{5}: Sieht gut aus. & \textbf{5}: sieht gut aus. \textit{Potential for Improvement:} nein. & \textbf{5}: Sehr hilfreich. \\
    \midrule
    8 & Softwareentwickler & \textbf{3}: - man sieht nicht, wie groß einzelne Cluster sind - das Diagramm wird nicht "vorsortiert", man muss als Nutzer selbst die Informationen aus der Heatmap herauslesen. & \textbf{4}: > Ziel dieses Features ist es, dass Use Case Admins erkennen können bei welchen Themen, welcher Aufgabentyp hauptsächlich zum Einsatz kommt - wird erfüllt, würde aber bspw. auch erfüllt werden durch ein Stacked Bar Chart mit einer Bar pro Use Case, wo man auch leicht die dicksten Teile erkennt, könnte man auch sortieren nach der Häufigkeit der Einsatzarten > Hierdurch sollen sie abschätzen können, welche Dokumentenbasen, wie verwendet werden, um diese für ihren Einsatzzweck optimieren zu können. gut. \textit{Potential for Improvement:} die oben genannten anderen Varianten für die Darstellung. & \textbf{4}: ich denke, das Gesamtziel ist leichter erreicht durch die vorherigen Dinge. Einzeln die Fragearten und einzeln die Dokumentenqualität. Ich frage mich, welchen Mehrwert es durch diese Kombination (wirklich) gibt. \\
    \midrule
    9 & Softwareentwickler & \textbf{5} & \textbf{5} & \textbf{3}: Finde den Insight auch weniger spannend als die ersten Features. \\
    \midrule
    10 & Developer & \textbf{3}: Gibt es eine Beschränkung für die Anzahl von Clustern? Das kann sehr schnell sehr groß und unübersichtlich werden. Statt Gesamtcluster vlt besser Anzahl Cluster oder Cluster Gesamt? & \textbf{5} & \textbf{3}: Ist vielleicht eher eine Statistik fürs Analytics Dashboard? Also klar, da gibt es kein Clustering, aber was will ich mit dem Wissen verbessern? \\
\end{longtable}

\clearpage

% -- Table 6: Quality by Task Type --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm} >{\RaggedRight}p{3.5cm}}
    \caption{Feedback on the Feature: Quality by Task Type}
    \label{tab:feedback_quality_by_task}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endfirsthead
    \caption[]{Feedback on the Feature: Quality by Task Type (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Is Intuitive to Use (Justification)} & \textbf{Fulfills Functionality (Potential for Improvement)} & \textbf{Provides Added Value (Justification)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{5} & \textbf{5} & \textbf{5} \\
    \midrule
    2 & Product Owner & \textbf{3}: Siehe vorhergehender Reiter (mehr Legenden, Beschreibungen). & \textbf{5} & \textbf{5} \\
    \midrule
    3 & Customer Success Manager & \textbf{4} & \textbf{5} & \textbf{5} \\
    \midrule
    4 & Manager & \textbf{4} & \textbf{4}: \textit{Potential for Improvement:} Ich würde für diese, wie auch alle andere Diagramme noch so ein LLM-generated "Statement" zur Interpretation der Diagramme anfügen. Sowas wie "Die relevanz der Antworten ist über die letzten 3 Tage von ...\% auf ...\% gestiegen." & \textbf{5} \\
    \midrule
    5 & Entwickler/TPO & \textbf{4} & \textbf{3}: Glaube zumindest in dem Screenshot kann man nicht allzuviel rauslesen. Glaube hier wäre auch wieder die Timeline weniger interessant und eher das "big picture", also zB einfach Balkendiagramm, wo man die verschiedenen Aufgabentypen einfacher vergleichen kann. (also zB Durchschnitt im ausgewählten Zeitraum). \textit{Potential for Improvement:} siehe oben. & \textbf{4}: Kann schon ganz gut helfen bei der Einschätzung der Qualität. \\
    \midrule
    6 & Developer & \textbf{5}: sinnvoll und gut gelöst. & \textbf{5}: sinnvoll und gut gelöst. & \textbf{5}: finde ich widerum sehr sinnvoll. \\
    \midrule
    7 & Software Entwickler & \textbf{3}: same, graph könnte verwirrend sein, ein einzelner score ist gut. & \textbf{5}: sieht gut aus. \textit{Potential for Improvement:} nein. & \textbf{5}: Hilfreich. \\
    \midrule
    8 & Softwareentwickler & \textbf{5} & \textbf{5}: > welche Aufgabentypen im Zeittrend, wie gut beantwortet werden SEHR GUT. & \textbf{5} \\
    \midrule
    9 & Softwareentwickler & \textbf{3}: Man muss viel händisch einstellen und sieht nicht viel auf einmal. & \textbf{5} & \textbf{4} \\
    \midrule
    10 & Developer & \textbf{3}: Fehlende Erklärung für Wert. Welcher Wert ist schlecht, welcher gut? Welchen Wert bekommt eine schlechte Antwort? Was kann der Nutzer verbessern? & \textbf{5} & \textbf{3} \\
\end{longtable}

\clearpage

% -- Table 7: General Feedback --
\begin{longtable}{l >{\RaggedRight}p{3cm} >{\RaggedRight}p{10cm}}
    \caption{General Feedback and Additional Feature Requests}
    \label{tab:general_feedback}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Overall Feedback and Suggestions} \\
    \midrule
    \endfirsthead
    \caption[]{General Feedback and Additional Feature Requests (Continued)}\\
    \toprule
    \textbf{\#} & \textbf{Position} & \textbf{Overall Feedback and Suggestions} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot

    1 & Entwickler & \textbf{Overall Value (5/5):} Absolut. Ich glaube das Thema ist mega wichtig, um neue Features zu identifizieren um die Pipeline zu verbessern und so viel mehr. \\
    \midrule
    2 & Product Owner & \textbf{Overall Value (5/5):} Wird sehr helfen beim Verständnis, auch wie Nutzer mit den Daten arbeiten. Besonders spannend werden dann die Qualitätsbewertungen. \textbf{Requests:} Erweiterung auf alles, was Nutzer im Wingman anfragen können. \textbf{Additional Comments:} Schöne Umsetzung, bin gespannt, wie sich das im Wingman macht :) \\
    \midrule
    3 & Customer Success Manager & \textbf{Overall Value (4/5):} Ich finde die Umsetzung mit Werten, Grafiken, Erklräungen und Datenpunkten sehr gut umgesetzt! \textbf{Requests:} Häufiger Bezug zu Clusterings oder Sources herstellen, weil die Zahlen dann auf Wissenpakete oder sogar Ordnerstrukturen gemached werden können. Daraus lassen sich noch viel weitere Folge To-Dos ableiten, die tatsächlich zu einer Verbesserung aktiv beitragen. Je nach Ergebnis: Mögliche Optimierungsmöglichkeiten anzeigen: Geringe Datenqualität -> Doku Data Maintanance; Pipeline-Optimierungen, Nutzerschulungen sollten angeboten werden; ... \textbf{Additional Comments:} Tendenziell positive Bewertungsmethoden wählen, die nicht für eine Verschlechterung der Datenqualität-Wahrnehmung sorgen. \\
    \midrule
    4 & Manager & \textbf{Overall Value (5/5):} Super hilfreich. \textbf{Requests:} S.o. - Ich würde für jedes Diagram und jede seite noch so ein LLM-generated "Statement" zur Interpretation der Diagramme anfügen. Sowas wie "Die relevanz der Antworten ist über die letzten 3 Tage von ...\% auf ...\% gestiegen." \textbf{Additional Comments:} Mega cool! DANKE und sieht sehr stark aus. \\
    \midrule
    5 & Entwickler/TPO & \textbf{Overall Value (5/5):} Glaube das kann super hilfreich sein, um den Wingman zu verbessern (für die jeweiligen Projekte). \textbf{Requests:} Vielleicht noch optionale Gruppierung per Knowledge Asset und nicht nur für den ganzen Use Case. \textbf{Additional Comments:} Mach mal die Dashboards (feedback und analytics) auch so geil in Vue. \\
    \midrule
    6 & Developer & \textbf{Overall Value (5/5):} Evaluation ist super wichtig. \textbf{Requests:} Schwierig nh Balance zwischen Kosten und Nutzen zu finden ... bei einem stärkeren Modell schießen die Kosten halt schon in die Höhe in Richtung +100\% und dafür bekommt man halt "nur" LLM-rated Zeug zurück ... aber ist auf jeden Fall sehr schick dass wir nh Evaluation haben. \textbf{Additional Comments:} Abgesehen davon finde ich die Diagramme extrem hübsch hahahaha und ohne LLM / mit Annotationen geht halt so nh Evaluation nicht wirklich für so vielfältige NLP Tasks die unser Wingman bearbeiten muss - da bin ich bei dir. Also ist schon gut gelöst sag ich dir wie's ist. \\
    \midrule
    7 & Software Entwickler & \textbf{Overall Value (5/5):} Nothing to add. \textbf{Requests:} Wenigere evaluation metrics, wenigere graphen/ separate advanced view. \textbf{Additional Comments:} Sieht insgesamt gut aus, gute arbeit :). \\
    \midrule
    8 & Softwareentwickler & \textbf{Overall Value (5/5):} Coole Analyse und vor allem schönes Interface. \textbf{Requests:} ich meine gesehen zu haben, dass an manchen Stellen (bspw. der Heatmap?) nicht alle Texte übersetzt worden sind, bpsw. Outliers statt Ausreißer und ggf. noch weitere Texte. \textbf{Additional Comments:} die Diagramme sehen top aus und Interaktion ist super! \\
    \midrule
    9 & Softwareentwickler & \textbf{Overall Value (4/5):} Es bietet gute Einblicke vor allem wie Admins ihre Daten verbessern können. Die Features weiter hinten in der Liste bieten meiner Meinung nach etwas weniger Mehrwert. \\
    \midrule
    10 & Developer & \textbf{Overall Value (4/5):} \textbf{Requests:} Mehr Informationen für die Diagramme, mehr Handlungsempfehlungen für die Nutzer, mehr Information über die Berechnungen und Ermittlungen der Werte und wie man sie interpretieren muss. \textbf{Additional Comments:} Schön gestaltet, gutes Layout, übersichtlich. \\
\end{longtable}
\chapter*{Image Credits}\label{section:image_credits}
\addcontentsline{toc}{chapter}{Image Credits} % Optional: Adds this section to the Table of Contents

The following icons have been used in diagrams in the thesis:

\begin{itemize}
    \item From \textbf{SVG Repo}:
    \begin{itemize}
        \item Cloud Scheduler SVG Vector: \url{https://www.svgrepo.com/svg/375384/cloud-scheduler}
        \item User SVG Vector: \url{https://www.svgrepo.com/svg/535711/user}
        \item Google Cloud Run SVG Vector: \url{https://www.svgrepo.com/svg/353807/google-cloud-run}
    \end{itemize}
    
    \item From \textbf{Worldvectorlogo}:
    \begin{itemize}
        \item Google BigQuery Logo: \url{https://worldvectorlogo.com/logo/google-bigquery-logo-1}
    \end{itemize}

    \item From \textbf{TechIcons}:
    \begin{itemize}
        \item Vue.js Icon: \url{https://techicons.dev/icons/vuejs}
    \end{itemize}
\end{itemize}
\printbibliography
\newpage
\mbox{}
\end{document}